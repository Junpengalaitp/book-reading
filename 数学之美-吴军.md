### 前言 ###
数学一词在西方源于古希腊语，意思是通过学习获得的知识。从这个角度来说，早期的数学涵盖的范围比今天的的数学要广的多，和人类的生活也更接近。
数学演化的过程，实际上是将我们生活中遇到的具体事物极其运动的规律不断抽象化的过程。

### 第一章 文字和语言 vs 数字和信息 ###
1948年，香农博士（Claude Elwood Shannon ，1916年4月30日—2001年2月24日）提出了信息论，将数学和信息系统联系了起来。在此之前，数学和语言学几乎没有任何交集。
拿破仑于1798年在埃及的罗赛塔（Rosetta）发现了一块破碎的古埃及石碑，上面有三种语言：埃及象形文字、埃及的拼音文字和古希腊文。罗赛塔的破译让人类了解了5000年前的埃及历史，所以今天有很多翻译软件和服务都命名成Rosetta。
1. 信息的冗余是信息安全的保障。
2. 语言的数据（语料），是机器翻译的基础。

### 第二章 自然语言处理 ###
基于统计的自然语言处理方法，在数学和模型上的通信是相通的，甚至就是相通的。因此，在数学意义上的自然语言处理又和语言的初衷--通信联系到一起了。但是，科学家们花了几十年时间才认识到这个联系。

### 第三章 统计语言模型 ###
计算机处理自然语言，一个基本的问题就是为上下文相关特性建立数学模型。
统计语言模型在形式上非常简单，也容易理解。但是里面的学问很深，一个专家可以在这方面研究很多年，数学的魅力就是将这些复杂的问题简单化。

### 第四章 谈谈分词 ###
中文的分词通过统计语言模型得到了解决，并且用到了其他语言和手写的罗马体语言中（手写体中和中文一样没有空格）

### 第五章 隐马尔可夫模型 ###
隐马尔可夫模型（Hidden Markov Model）其实是美国数学家鲍姆（Leonard E. Baum）等人在20世纪六七十年代提出的。
19世纪，概率论的发展从对随机变量的研究发展到对随机变量的时间序列，即随机过程的研究，这是一个质的飞跃，也大大增加了复杂度。因为随机过程有2个维度的：每个状态本身，和周围其他状态。马尔可夫为了简化，即随机过程中任意状态的概率分布，只和它前一个的状态有关。

### 第六章 信息的度量与作用 ###
信息量的度量直到1948年香农提出信息熵的概念，才有量化信息的方法。
信息量与不确定性有着直接的关系，如果要弄清楚一件非常不确定的事情，需要大量的信息，反之，需要的信息较少。
信息熵是整个信息论的基础，它对于通信、数据压缩、自然语言处理都有很大的指导意义。其物理含义是对一个系统不确定性的度量，与热力学中的熵的概念有相似之处。

### 第七章 贾里尼克和现代语言处理 ###
在当今物欲横流的社会，学术界浮躁，年轻人焦虑，少数有远大志向的年轻人实际上是非常孤独的。

### 第八章 简单之美 布尔代数和搜索引擎 ###
建立一个搜索引擎大致需要做这样几件事：自动下载尽可能多的网页；建立快速有效的索引；根据相关性对网页进行公平准确的排序。
布尔代数将数学和逻辑合二为一，开创了今天的数字化时代。
Truth is ever to be found in simplicity, and not in the multiplicity and confusion of things

### 第九章 图论和网络爬虫 ###
互联网虽然很复杂，但其实就是一张大图而已，用图的遍历算法可以从任何一个网页出发，自动访问每一个网页并保存他们。

### 第十章 PageRank ###
如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，那么它的排名就高。

### 第十一章 如何确定网页和查询相关性 ###
早期主要依靠技术与算法，当今由于有了大量用户点击数据，对搜索相关性贡献度最大的是根据用户对常见搜索点击网页的结果得到的概率模型。
1. 完备的索引
2. 对网页质量的度量
3. 用户偏好
4. 确定一个网页和某个查询的相关性方法
   
TF-IDF(Term Frequency / Inverse Document Frequency)
TF: 关键词出现的次数/网页总字数
IDF: 逆文本频率指数log(D/Dw), 特定条件下关键词的概率分布的交叉熵
Stop Word: 在度量相关性时不考虑频率的词

### 第十二章 有限状态机和动态规划 ###
有限状态机是一个特殊的有向图，有一个开始和终止状态，以及若干中间状态。每一条弧上带有一个从当前状态到下个状态的条件。

### 第十三章 Google AK-47的设计者 ###
先帮助用户解决80%的问题，再慢慢解决剩下的20%的问题。

### 第十四章 余弦定理和新闻的分类 ###
先将新闻的文字变成一组可计算的数字，然后再设计一个算法计算出新闻间的相似程度。对新闻中所有的实体词，计算出它们的TF-IDF值，得到一个特征向量。
向量实际上是多维空间中从原点出发的有向线段，利于余弦计算出两个向量间的度数，就可以得到两篇新闻的相似度。

### 第十五章 矩阵运算和文本处理中的两个分类问题 ###
新闻量大的时候，两两计算出相似度，时间复杂度非常高（n^2）。使用奇异值分解（Singular Value Decomposition），可以一步到位地把所有新闻的相关性都算出来。
一个大矩阵，1000000 x 500000，共有5000亿个元素，计算起来非常耗时。奇异值分解，就是把这样一个大矩阵分解成3个小矩阵相乘。
1000000 x 100 X, 100 x 100 B, 100 x 500000 Y, 这三个矩阵总数只有1.5亿，不到原来的三分之一。

### 第十六章 信息指纹极其应用 ###
搜索引擎如果直接缓存url, 很浪费空间，需要一种算法将其映射到较短的长度。

### 第十七章 密码学的数学原理 ###
公钥和电子签名的方法，是今天大多数互联网安全协议的基础。具体有RSA算法，Rabin算法，ElGamal算法，椭圆曲线算法等，这些算法都有一些共同点。
1. 它们有两个完全不同的密钥，用来加密和解密。
2. 两个密钥数学上是关联的

### 第十八章 搜索引擎反作弊和权威性问题 ###
反作弊的第一条是要增强排序算法的抗噪声能力，第二是图论。
权威性和网页质量不同，它是要和搜索主题相关的。

### 第十九章 数学模型的重要性 ###
1. 一个正确的数学模型应该在形式上是简单的。
2. 一个正确的模型一开始可能不如一个精雕细琢过后的错误模型。
3. 大量准确的数据很重要。
4. 正确的模型也可能受噪声干扰。
   
### 第二十章 不要把鸡蛋放到一个篮子里 最大熵模型 ###
最大熵：保留全部的不确定性，将风险降到最小。对一个随机事件的概率分布进行预测时，预测应该满足所有已知条件，对未知的情况不作任何主观假设。
早期，由于最大熵计算量大，应用不是很广泛。第一个在实际信息处理中应用验证了最大熵模型的是帕拉提，他没有对模型进行近似处理，而是找到了几个最适合最大熵模型而计算量相对不太大的自然语言处理问题，比如词性标注和句法分析。

### 第二十一章 拼音输入法的数学原理 ###
汉字的输入过程本身就是人和计算机的通信，好的输入法会自觉或者不自觉地遵循通信的数学模型。当然要做出最有效的输入法，应该自觉使用信息论做指导。

### 第二十二章 自然语言处理的教父马库斯和他的优秀弟子们 ###
两大难题：可用于研究的统计数据不够；各国科学家使用的数据不同，论文结果无法相互比较。
马库斯利用自己的影响力，推动美国自然科学基金会和DARPA出资立项，联络了多所大学和研究机构，建立了数百个标准的语料库组织。

### 第二十三章 布隆过滤器 ###
使用Hash Table在大数据的场景下非常耗内存，而使用布隆过滤器只需要25%-12.5%的空间。
布隆过滤器是由一个很长的二进制向量和一系列随机映射函数。
假设有一亿个电子邮件地址需要储存，先建立一个16亿个比特位，即两亿字节的向量，然后将这16亿个比特位全部清零。对于每一个电子邮件地址X，用8个不同的随机数产生器（F1,F2,...,F8）产生8个信息指纹（f1,f2,...f8）。再用一个随机数生成器G把这8个信息指纹映射到1-16亿中的8个自然数g1,g2,...,g8。现在把8个位置的比特位全部设置为1。对这一亿个电子邮件地址都进行这样的处理后，布隆过滤器就建成了。
检测一个电子邮件地址时，用相同的8个随机数产生器（F1,F2,...,F8)对这个地址产生8个信息指纹s1,s2,...s8,然后将这8个指纹对应到布隆过滤器的8比特，分别是t1,t2,...t8。如果此电子邮件在黑名单中，显然，t1,t2,...t8对应的8比特值一定是1.这样，任何黑名单中的地址都能准确的发现。
布隆过滤器不会漏掉任何一个黑名单地址，但是有极小的可能性（小于万分之一）过滤掉不在黑名单中的地址。常见的解决办法是建立一个小的白名单，储存那些可能被误过滤的地址。

### 第二十四章 马尔可夫链的扩展 贝叶斯网络 ###
一个有向图中，如果马尔可夫假设成立，即每个状态只和它直接相连的状态有关，间接相连的没有关系，他就是贝叶斯网络。

### 第二十五章 条件随机场、文法分析及其他 ###
自然语言处理的句子分析（Sentence Parsing）一般是根据文法对一个句子进行分析，建立这个句子的语法树，即句法分析（Syntactic Parsing），有时也指对一个句子中各成分的语义进行分析，得到对这个句子语义的一种描述，即语义分析（Semantic Parsing）。

### 第二十六章 维特比和维特比算法 ###
维特比算法是一个特殊但是运用最广泛的动态规划算法，针对一个特殊的图--篱笆网络（Lattice）的有向图最短路径而提出的。凡是使用隐马尔可夫模型描述的问题都可以用它来解码。

### 第二十七章 上帝的算法 期望值最大算法 ###
EM算法只需要有一些训练数据，定义一个最大化函数，剩下的事情就可以交给计算机了。经过若干次迭代，模型就训练好了。

### 第二十八章 逻辑回归和搜索广告 ###
逻辑回归模型是一种将影响概率的不同因素结合在一起的指数模型。和很多指数模型（例如最大熵模型）一样，它们的训练方法相似，都可以采用通用迭代算法GIS和改进的迭代算法IIS来实现。除了在信息处理中的应用，逻辑回归模型还广泛应用于生物统计。

### 第二十九章 各个击破算法和GCP的基础 ###
云计算的关键之一是，如何把一个非常大的计算问题，自动分解到许多计算能力不是很强大的计算机上，共同完成。Google给出的方案是MapReduce，其根本原理是分治法。

### 第三十章 Google大脑和人工神经网络 ###
人工神经网络和贝叶斯网络的共同点：
1. 它们都是有向图，每一个节点只取决于前一级的节点，遵从马尔可夫假设。
2. 训练方法相似。
3. 多模式分类问题的效果上相似。
4. 训练计算量都非常大。
区别：
1. 人工神经网络在结构上是完全标准化的，而贝叶斯网络更灵活。
2. 虽然神经元函数为非线性函数，但是各个变量只能先进行线性组合，最后对一个变量进行非线性变换，因此用计算机实现起来比较容易。而在贝叶斯网络中，变量可以组合成任意的函数，毫无限制，在获得灵活性的同时，也增加了复杂性。
3. 贝叶斯网络更容易考虑上下文前后的相关性，因此可以得到一个输入的序列，比如将一段语音识别成文字，或者翻译一段文字。而人工神经网络的输出相对孤立，它可以识别一个个字，但是很难处理一个序列，因此它的主要应用是估计一个概率模型的参数，比如语音识别中声学模型参数的训练、机器翻译中语言模型参数的训练等，而不是作为解码器。

### 第三十一章 区块链的数学基础 ###
在完全开放信息的社会里，想保护私有信息，必须有一套不对称的机制，做到在特定授权的情况下，不需要拥有信息也能使用信息；在不授权信息访问权限时，也能验证信息。比特币证明了利用区块链能同时做到这两件事情。
