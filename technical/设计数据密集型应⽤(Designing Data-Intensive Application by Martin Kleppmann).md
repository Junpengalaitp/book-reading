# 第一部分
* 数据密集型应用设计的基本思想

## 第一章 可靠性、可扩展性、可维护性
* 数据密集型应用通常由提供通用功能的标准组件构建而成
  * 存储数据: 数据库
  * 减少昂贵操作的次数：缓存
  * 搜索、过滤数据：搜索索引
  * 跨进程通信：流处理
  * 定期处理累积的大批量数据：批处理


### 关于数据系统的思考
* 新的存储和数据处理工具不断出现，它们针对不同的场景进行优化，因此不在适合生硬地归入传统类别。
  * 缓存Redis可以作为消息队列
  * Kafka消息队列可以向数据库一样持久化消息
* 单个工具不足以满足应用程序的复杂以及严格的要求，总体工作被拆分成一系列能被单个工具高效完成的任务，通过应用代码将它们结合起来。
* 通过API提供多个工具组合后的功能，组合后的工具时一个全新的，专用的数据系统，提供特定的保证。例如缓存在写入时会作废或更新，以便外部客户端获取一致的结果。

#### 可靠性(Reliability)
* 系统在困境(adversity)(硬件、软件故障，人为错误)中仍然可以正常工作(正确完成功能，并达到期望的水准)
* 可靠软件的典型期望
  * 表现出用户所期望的功能
  * 允许用户犯错，允许用户以出乎意料的方式使用软件
  * 在预期的负载和数据量下，性能满足要求
  * 能防止未经授权的访问和滥用
* 故障(fault): 造成错误的原因
* 容错/韧性(fault-tolerant/resilient): 能预料并应对故障的系统特性
* 失效(failure): 故障为系统的部分状态偏离其标准，失效是整个系统停止服务
* 故意触发来提高故障率是有意义的，可以确保容错机制不断运行并接受考验
* 相对于阻止错误，我们应更倾向于容忍错误

#### 硬件故障
* 增加单个硬件的冗余度
#### 软件错误
* 硬件故障一般是随机的、相互独立的，大量硬件组件不太可能同时失效。
* 系统性错误(systematic error)
  * 难以预料、跨节点相关的，可能造成链式失效
  * 接受特定错误输入后导致所有应用服务器实例崩溃的BUG
  * 失控进程会占用一些共享资源
  * 系统依赖的服务变慢或没有正确的响应
  * 级联故障，一个组件中的小故障触发另一个组件中的故障，进而触发更多的故障
* 预防方法
  * 仔细考虑系统中的假设和交互
  * 彻底的测试
  * 进程隔离
  * 允许进程崩溃并重启
  * 测量、监控、并分析生成环境中的系统行为
  * 如果系统能够提供一些保证(例如在一个消息队列中，进入与发出的消息数量相等)，那么系统就可以在运行时不断自检，并在出现差异(discrepancy)时报警。
#### 人为错误
* 以最小化犯错机会的方式设计系统
* 将容易犯错的地方与可能失效的地方解耦，提供一个功能齐全的非生产环境沙箱
* 在各个层次进行彻底的测试
* 允许从人为错误中简单快速地恢复，以最大限度地减少失效情况带来的影响。例如快速回滚配置变更，分批发布新代码，并提供数据重算工具
* 配置详细和明确的监控，比如性能指标和错误率。在其他工程学科中就是遥测(telemetry)
* 良好的管理实践与充分的培训
#### 可靠性有多重要？
* 商务应用中的错误会导致生产力损失（也许数据报告不完整还有法律风险），而电商网站的终端则可能导致收入和声誉的巨大损失。

#### 可扩展性(Scalability)
* 系统现在运行正常，不代表未来也能可靠运行。服务降级(degradation)的一个常见原因是负载增加。

##### 描述负载
* 由合理的办法应对系统的增长(数据量、流量、复杂性)
* 负载参数(load parameters)
  * 每秒Web请求数量
  * 数据库读写比率
  * 同时活跃的用户数量
  * 缓存命中率
  * ...

##### 描述性能
* 增加负载参数并保持系统资源不变时，系统性能收到的影响？
* 增加负载参数并保持性能不变时，需要增加多少系统资源？
* 吞吐量(throughput): 每秒可处理的记录数量，或者在特定规模数据集上运行作业的总时间。通常用在像Hadoop这样的批处理系统上。
* 响应时间(response time): 客户端发送请求到收到响应的时间，通常用于在线系统上。包括网络和排队延时。
  * 响应时间的尾部延时非常重要，因为它直接影响用户体验
    * 响应时间最慢的客户往往是数据最多的客户，也就是最有价值的客户。
    * Amazon观察到：响应时间增加100毫秒，销售量就会减少1%
  * 百分位点通常用于服务基本目标(SLO, service level objectives)和服务级别协议(SLA, service level agreements)
    * 例如服务响应时间的中位数小于200毫秒，且99.9百分位点低于1秒，则认为服务工作正常。
  * 排队延时(queueing delay)通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务，所以只需少量缓慢请求就能阻塞后续请求的处理(头部阻塞 head-of-line blocking)
* 当一个请求需要多个后端请求组合完成时，单个后端慢请求就会拖慢整个终端用户的请求

##### 应对负载的方法
* 纵向扩展(scaling up)/垂直扩展(vertical scaling)
* 横向扩展(scaling out)/水平扩展(horizontal scaling)
* 弹性扩展：检测到负载时自动增加计算资源
* 一个良好适配应用的可扩展架构，是围绕这假设(assumption)建立的
  * 哪些操作是常见的？
  * 哪些操作是罕见的？


#### 可维护性(Maintainability)
* 不同的人员在不同的周期都能在系统上高效地工作
* 可操作性(Operability)：人生苦短，关爱运维
  * 运维的职责
    * 监控系统运行情况，并在服务状态不佳时快速恢复服务
    * 跟踪问题的原因
    * 及时更新软件和平台
    * 了解系统间相互作用，以便在异常造成损失前进行规避
    * 预测未来问题，并在问题出现之前加以解决(如容量规划)
    * 建立部署，配置和管理方面的良好实践，编写相应工具
    * 执行复杂的维护任务，例如将应用程序跨平台迁移
    * 当配置变更时，维持系统的安全性
    * 定义工作流程，使运维操作可预测，并保持生产环境稳定
    * 维持组织对系统的了解
  * 良好的可操作性意外着更轻松的日常工作，进而运维团队能专注于高价值的事情。数据系统可以通过各种方式使日常任务更轻松
    * 通过良好的监控，提供对系统内部状态和运行时行为的可见性(visibility)
    * 为自动化提供良好支持，将系统与标准化工具相集成
    * 避免依赖单台机器(在整个系统继续不间断运行的情况下允许机器停机维护)
    * 提供良好的文档和易于理解的操作模型
    * 提供良好的默认行为，但也允许管理员自由覆盖默认值
    * 有条件时进行自我修复，但也允许管理员手动控制系统状态
    * 行为可预测，最大限度减少意外

* 简单性(Simplicity)：管理复杂度
  * 消除额外的(accidental)复杂度: 由具体实现中出现，而非用户视角的问题本身复杂度。
  * 使用抽象，好的抽象可以将大量实现细节隐藏在一个干净，简单易懂的外观下面。也可以提升代码的复用性，提高效率。

* 可演化性(evolability)：拥抱变化
  * 系统的需求处于常态的变化中
    * 新的事实
    * 意想不到的应用场景
    * 业务优先级变化
    * 用户要求的新功能
    * 新平台取代旧平台
    * 法律法规发生变化
    * 系统增长迫使架构变化
    * ...
  * 敏捷开发模式
  * TDD
  * Refactoring

### 本章小结
* 本章探讨了一些关于数据密集型应用的基本思考方式。这些原则将指导我们阅读本书的其余部分，那里将会深入技术细节。
* 一个应用必须满足各种需求才有用
  * 功能性需求
  * 非功能性需求
    * 通用属性
    * 安全性
    * 合规性
    * 兼容性
    * 可靠性(本章内容)
      * 即使发生软件、硬件或人为故障，系统也能正常工作。容错技术可以对终端用户隐藏某些类型的故障。
    * 可扩展性(本章内容)
      * 即使在负载增加的情况下也保持性能的策略
    * 可维护性(本章内容)
      * 同时也是关于工程师和运维团队的生活质量的。良好的抽象可降低复杂度，并使系统易于修改和适应新的应用场景。


## 第二章 数据模型与查询语言
### 关系模型与文档模型
#### NoSQL的诞生
#### 对象关系不匹配
#### 多对一和多对多的关系
#### 文档数据库是否在重蹈覆辙？
* 网络模型
* 关系模型
* 与文档数据库相比
#### 关系型数据库与文档数据库在今日的对比
* 哪个数据模型更方便写代码？
* 文档模型中的架构灵活性
* 查询的数据局部性
* 文档和关系数据库的融合
### 数据查询语言
#### Web上的声明式查询
#### MapReduce查询
### 图数据模型
#### 属性图
#### Cypher查询语言
#### SQL中的图查询
#### 三元组存储和SPARQL
* 语义网络
* RDF数据模型
#### SPARQL查询语言
#### 基础：Datalog
* 数据模型可能是软件开发中最重要的部分了，因为它们的影响如此深远：不仅仅影响着软件的编写⽅
式，⽽且影响着我们的解题思路。
* 多数应⽤使⽤层层叠加的数据模型构建。对于每层数据模型的关键问题是：它是如何⽤低⼀层数据模型
来表示的？
  * 作为⼀名应⽤开发⼈员，你观察现实世界（⾥⾯有⼈员，组织，货物，⾏为，资⾦流向，传感器
等），并采⽤对象或数据结构，以及操控那些数据结构的API来进⾏建模。那些结构通常是特定于
应⽤程序的。
  * 当要存储那些数据结构时，你可以利⽤通⽤数据模型来表示它们，如JSON或XML⽂档，关系数据
库中的表、或图模型。
  * 数据库软件的⼯程师选定如何以内存、磁盘或⽹络上的字节来表示JSON/XML/关系/图数据。这类
表示形式使数据有可能以各种⽅式来查询，搜索，操纵和处理。
  * 在更低的层次上，硬件⼯程师已经想出了使⽤电流，光脉冲，磁场或者其他东⻄来表示字节的⽅
法。
* ⼀个复杂的应⽤程序可能会有更多的中间层次，⽐如基于API的API，不过基本思想仍然是⼀样的：每个
层都通过提供⼀个明确的数据模型来隐藏更低层次中的复杂性。这些抽象允许不同的⼈群有效地协作。
* 掌握⼀个数据模型需要花费很多精⼒（想想关系数据建模有多少本书）。即便只使⽤⼀个数据模型，不
⽤操⼼其内部⼯作机制，构建软件也是⾮常困难的。然⽽，因为数据模型对上层软件的功能（能做什
么，不能做什么）有着⾄深的影响，所以选择⼀个适合的数据模型是⾮常重要的。

### 关系模型与⽂档模型
* 现在最著名的数据模型可能是SQL。它基于Edgar Codd在1970年提出的关系模型【1】：数据被组织成
关系（SQL中称作表），其中每个关系是元组（SQL中称作⾏)的⽆序集合。

#### NoSQL的诞⽣
* 需要比关系型数据库更好的扩展性，包括非常大的数据集或非常高的写入吞吐量
* 免费和开源
* 关系模型不能很好地支持一些特殊的查询
* 受挫于关系模型的限制性，渴望一种更具多动态性与表现力的数据模型
##### 混合持久化
* SQL和NoSQL混用

#### 对象关系不匹配
* 阻抗不匹配(impedance mismatch): 面向对象的代码存储在SQL表中需要一个笨拙的转换层。
* 对于⼀个像简历这样⾃包含⽂档的数据结构⽽⾔，JSON表示是⾮常合适的：参⻅例2-1。JSON⽐XML更
简单。⾯向⽂档的数据库（如MongoDB，RethinkDB，CouchDB和
Espresso）⽀持这种数据模型。

#### 多对⼀和多对多的关系
* 使用id而不是实际文本的好处
  * 各个简介之间样式和拼写统⼀
  * 避免歧义（例如，如果有⼏个同名的城市）
  * 易于更新——名称只存储在⼀个地⽅，如果需要更改（例如，由于政治事件⽽改变城市名称），很容易进⾏全⾯更新。
  * 本地化⽀持——当⽹站翻译成其他语⾔时，标准化的列表可以被本地化，使得地区和⾏业可以使⽤⽤户的语⾔来显示
  * 更好的搜索——例如，搜索华盛顿州的慈善家就会匹配这份简介，因为地区列表可以编码记录⻄雅图在华盛顿这⼀事实（从"Greater Seattle Area"这个字符串中看不出来）。
* 存储ID还是⽂本字符串，这是个副本（duplication）问题。当使⽤ID时，对⼈类有意义的信息（⽐如
单词：Philanthropy）只存储在⼀处，所有引⽤它的地⽅使⽤ID（ID只在数据库中有意义）。当直接存
储⽂本时，对⼈类有意义的信息会复制在每处使⽤记录中。
* 使⽤ID的好处是，ID对⼈类没有任何意义，因⽽永远不需要改变：ID可以保持不变，即使它标识的信息
发⽣变化。任何对⼈类有意义的东⻄都可能需要在将来某个时候改变——如果这些信息被复制，所有的
冗余副本都需要更新。这会导致写⼊开销，也存在不⼀致的⻛险（⼀些副本被更新了，还有些副本没有
被更新）。去除此类重复是数据库规范化（normalization）的关键思想。

#### ⽂档数据库是否在重蹈覆辙？
#### 关系型数据库与⽂档数据库在今⽇的对⽐
##### 哪个数据模型更⽅便写代码？
* 如果应⽤程序中的数据具有类似⽂档的结构（即，⼀对多关系树，通常⼀次性加载整个树），那么使⽤
⽂档模型可能是⼀个好主意。将类似⽂档的结构分解成多个表的关系技术可能导致繁琐的模式和不必要的复杂的应⽤程序代码。
* 但是，如果你的应⽤程序确实使⽤多对多关系，那么⽂档模型就没有那么吸引⼈了。通过反规范化可以
减少对连接的需求，但是应⽤程序代码需要做额外的⼯作来保持数据的⼀致性。
* 很难说在⼀般情况下哪个数据模型让应⽤程序代码更简单；它取决于数据项之间存在的关系种类。对于
⾼度相联的数据，选⽤⽂档模型是糟糕的，选⽤关系模型是可接受的，⽽选⽤图形模型是最⾃然的。
##### ⽂档模型中的架构灵活性
* ⼤多数⽂档数据库以及关系数据库中的JSON⽀持都不会强制⽂档中的数据采⽤何种模式。关系数据库的
XML⽀持通常带有可选的模式验证。没有模式意味着可以将任意的键和值添加到⽂档中，并且当读取
时，客户端对⽆法保证⽂档可能包含的字段。
* ⽂档数据库有时称为⽆模式（schemaless），但这具有误导性，因为读取数据的代码通常假定某种结
构——即存在隐式模式，但不由数据库强制执⾏。⼀个更精确的术语是读时模式（schema-on-read）（数据的结构是隐含的，只有在数据被读取时才被解释），相应的是写时模式（schema-on-write）（传统的关系数据库⽅法中，模式明确，且数据库确保所有的数据都符合其模式）
* 当由于某种原因（例如，数据是异构的）集合中的项⽬并不都具有相同的结构时,读时模式更具优势
  * 存在许多不同类型的对象，将每种类型的对象放在⾃⼰的表中是不现实的。
  * 数据的结构由外部系统决定。你⽆法控制外部系统且它随时可能变化。
##### 查询的数据局部性
* ⽂档通常以单个连续字符串形式进⾏存储，编码为JSON，XML或其⼆进制变体（如MongoDB的
BSON）。如果应⽤程序经常需要访问整个⽂档（例如，将其渲染⾄⽹⻚），那么存储局部性会带来性
能优势。如果将数据分割到多个表中（如图2-1所示），则需要进⾏多次索引查找才能将其全部检索出
来，这可能需要更多的磁盘查找并花费更多的时间。
* 局部性仅仅适⽤于同时需要⽂档绝⼤部分内容的情况。数据库通常需要加载整个⽂档，即使只访问其中
的⼀⼩部分，这对于⼤型⽂档来说是很浪费的。更新⽂档时，通常需要整个重写。只有不改变⽂档⼤⼩
的修改才可以容易地原地执⾏。因此，通常建议保持相对⼩的⽂档，并避免增加⽂档⼤⼩的写⼊。这些性能限制⼤⼤减少了⽂档数据库的实⽤场景。

##### ⽂档和关系数据库的融合

### 数据查询语⾔
#### Web上的声明式查询
#### MapReduce查询
* MapReduce既不是⼀个声明式的查询语⾔，也不是⼀个完全命令式的查询API，⽽是处于两者之间：查
询的逻辑⽤代码⽚断来表示，这些代码⽚段会被处理框架重复性调⽤。它基于 map （也称
为 collect ）和 reduce （也称为 fold 或 inject ）函数，两个函数存在于许多函数式编程语⾔中。

### 图数据模型
#### 属性图
* 在属性图模型中，每个顶点（vertex）包括：
  * 唯⼀的标识符
  * ⼀组出边（outgoing edges）
  * ⼀组⼊边（ingoing edges）
  * ⼀组属性（键值对）
* 每条边（edge）包括：
  * 唯⼀标识符
  * 边的起点/尾部顶点（tail vertex）
  * 边的终点/头部顶点（head vertex）
  * 描述两个顶点之间关系类型的标签
  * ⼀组属性（键值对）
* 重要特性
  * 任何顶点都可以有⼀条边连接到任何其他顶点。没有模式限制哪种事物可不可以关联。
  * 给定任何顶点，可以⾼效地找到它的⼊边和出边，从⽽遍历图，即沿着⼀系列顶点的路径前后移
动。
  * 通过对不同类型的关系使⽤不同的标签，可以在⼀个图中存储⼏种不同的信息，同时仍然保持⼀个
清晰的数据模型。

#### Cypher查询语⾔
#### SQL中的图查询
#### 三元组存储和SPARQL
* 在三元组存储中，所有信息都以⾮常简单的三部分表示形式存储（主语，谓语，宾语）。例如，三元组
(吉姆, 喜欢 ,⾹蕉)中，吉姆是主语，喜欢是谓语（动词），⾹蕉是对象。
* 三元组的主语相当于图中的⼀个顶点。⽽宾语是下⾯两者之⼀：
  1. 原始数据类型中的值，例如字符串或数字。在这种情况下，三元组的谓语和宾语相当于主语顶点上
  的属性的键和值。例如， (lucy, age, 33) 就像属性 {“age”：33} 的顶点lucy。
  2. 图中的另⼀个顶点。在这种情况下，谓语是图中的⼀条边，主语是其尾部顶点，⽽宾语是其头部顶
  点。例如，在 (lucy, marriedTo, alain) 中主语和宾语 lucy 和 alain 都是顶点，并且谓语
  marriedTo 是连接他们的边的标签。
#### 语义⽹络
#### RDF数据模型
#### SPARQL查询语⾔

### 本章⼩结
数据模型是⼀个巨⼤的课题，在本章中，我们快速浏览了各种不同的模型。我们没有⾜够的空间来详细
介绍每个模型的细节，但是希望这个概述⾜以激起你的兴趣，以更多地了解最适合你的应⽤需求的模
型。
在历史上，数据最开始被表示为⼀棵⼤树（层次数据模型），但是这不利于表示多对多的关系，所以发
明了关系模型来解决这个问题。最近，开发⼈员发现⼀些应⽤程序也不适合采⽤关系模型。新的⾮关系
型“NoSQL”数据存储在两个主要⽅向上存在分歧：
1. ⽂档数据库的应⽤场景是：数据通常是⾃我包含的，⽽且⽂档之间的关系⾮常稀少。
2. 图形数据库⽤于相反的场景：任意事物都可能与任何事物相关联。
这三种模型（⽂档，关系和图形）在今天都被⼴泛使⽤，并且在各⾃的领域都发挥很好。⼀个模型可以
⽤另⼀个模型来模拟 — 例如，图数据可以在关系数据库中表示 — 但结果往往是糟糕的。这就是为什么
我们有着针对不同⽬的的不同系统，⽽不是⼀个单⼀的万能解决⽅案。
⽂档数据库和图数据库有⼀个共同点，那就是它们通常不会为存储的数据强制⼀个模式，这可以使应⽤
程序更容易适应不断变化的需求。但是应⽤程序很可能仍会假定数据具有⼀定的结构；这只是模式是明
确的（写⼊时强制）还是隐含的（读取时处理）的问题。
每个数据模型都具有各⾃的查询语⾔或框架，我们讨论了⼏个例⼦：SQL，MapReduce，MongoDB的
聚合管道，Cypher，SPARQL和Datalog。我们也谈到了CSS和XSL/XPath，它们不是数据库查询语⾔，
⽽包含有趣的相似之处。
虽然我们已经覆盖了很多层⾯，但仍然有许多数据模型没有提到。举⼏个简单的例⼦：
使⽤基因组数据的研究⼈员通常需要执⾏序列相似性搜索，这意味着需要⼀个很⻓的字符串（代表
⼀个DNA分⼦），并在⼀个拥有类似但不完全相同的字符串的⼤型数据库中寻找匹配。这⾥所描
述的数据库都不能处理这种⽤法，这就是为什么研究⼈员编写了像GenBank这样的专⻔的基因组
数据库软件的原因【48】。
粒⼦物理学家数⼗年来⼀直在进⾏⼤数据类型的⼤规模数据分析，像⼤型强⼦对撞机（LHC）这样
的项⽬现在可以⼯作在数百亿兆字节的范围内！在这样的规模下，需要定制解决⽅案来阻住硬件成
本的失控【49】。
全⽂搜索可以说是⼀种经常与数据库⼀起使⽤的数据模型。信息检索是⼀个很⼤的专业课题，我们
不会在本书中详细介绍，但是我们将在第三章和第三章中介绍搜索索引

## 第三章 存储与检索
### 驱动数据库的数据结构
* 为了高效查找数据库中特定键的值，我们需要⼀个数据结构：索引（index）。它保存⼀些额外的元数据作为路标，帮助你找到想要的数据。如果您想在同⼀份数据中以⼏种不同的⽅式进⾏搜索，那么你也许需要不同的索引，建在数据的不同部分上。
* 索引是从主数据衍⽣的附加（additional）结构。许多数据库允许添加与删除索引，这不会影响数据的
内容，它只影响查询的性能。维护额外的结构会产⽣开销，特别是在写⼊时。写⼊性能很难超过简单地
追加写⼊⽂件，因为追加写⼊是最简单的写⼊操作。任何类型的索引通常都会减慢写⼊速度，因为每次
写⼊数据时都需要更新索引。

#### 哈希索引
* 键值存储与在⼤多数编程语⾔中可以找到的字典（dictionary）类型⾮常相似，通常字典都是⽤散列映
射（hash map）（或哈希表（hash table））实现的。
* 但是，哈希表索引也有局限性：
  * 散列表必须能放进内存。如果你有⾮常多的键，原则上可以在磁盘上保留⼀个哈希映射，不幸的是磁盘哈希映
    射很难表现优秀。它需要⼤量的随机访问I/O，当它变满时增⻓是很昂贵的，并且散列冲突需要很
    多的逻辑。
  * 范围查询效率不⾼。例如，您⽆法轻松扫描kitty00000和kitty99999之间的所有键——您必须在散
列映射中单独查找每个键。

#### SSTables和LSM树
* 键值对的序列按键排序, 排序字符串表（Sorted String Table），简称SSTable
  * 合并段是简单⽽⾼效的，即使⽂件⼤于可⽤内存。这种⽅法就像归并排序算法中使⽤的⽅法⼀样, 产⽣新的合并段⽂件，也按键排序。
  * 为了在⽂件中找到⼀个特定的键，你不再需要保存内存中所有键的索引。
##### 构建和维护SSTables
* 写⼊时，将其添加到内存中的平衡树数据结构（例如，红⿊树）。这个内存树有时被称为内存表
（memtable）。
* 当内存表⼤于某个阈值（通常为⼏兆字节）时，将其作为SSTable⽂件写⼊磁盘。这可以⾼效地完
成，因为树已经维护了按键排序的键值对。新的SSTable⽂件成为数据库的最新部分。当SSTable
被写⼊磁盘时，写⼊可以继续到⼀个新的内存表实例。
* 为了提供读取请求，⾸先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下⼀个较旧
的段中找到该关键字。
* 有时会在后台运⾏合并和压缩过程以组合段⽂件并丢弃覆盖或删除的值。
##### ⽤SSTables制作LSM树
* 最初这种索引结构是由Patrick O'Neil等⼈描述的。在⽇志结构合并树（或LSM树）的基础上，建
⽴在以前的⼯作上⽇志结构的⽂件系统。基于这种合并和压缩排序⽂件原理的存储引擎通常被称
为LSM存储引擎。
* Lucene是Elasticsearch和Solr使⽤的⼀种全⽂搜索的索引引擎，它使⽤类似的⽅法来存储它的词典
。全⽂索引⽐键值索引复杂得多，但是基于类似的想法：在搜索查询中给出⼀个单词，找到
提及单词的所有⽂档（⽹⻚，产品描述等）。这是通过键值结构实现的，其中键是单词（关键词
（term）），值是包含单词（⽂章列表）的所有⽂档的ID的列表。在Lucene中，从术语到发布列表的
这种映射保存在SSTable类的有序⽂件中，根据需要在后台合并。
##### 性能优化
* 当查找数据库中不存在的键时，LSM树算法可能会很慢：您必须检查内存表，然后将这些段⼀直回到最⽼的（可能必须从磁盘读取每⼀
个），然后才能确定键不存在。为了优化这种访问，存储引擎通常使⽤额外的Bloom过滤器。
（布隆过滤器是⽤于近似集合内容的内存⾼效数据结构，它可以告诉您数据库中是否出现键，从⽽为不
存在的键节省许多不必要的磁盘读取操作。
* 还有不同的策略来确定SSTables如何被压缩和合并的顺序和时间。最常⻅的选择是⼤⼩分层压实。

#### B树
* 使⽤最⼴泛的索引结构在1970年被引⼊，不到10年后变得“⽆处不在”，B树经受了时间的考验。在⼏乎所
有的关系数据库中，它们仍然是标准的索引实现，许多⾮关系数据库也使⽤它们。
* 像SSTables⼀样，B树保持按键排序的键值对，这允许⾼效的键值查找和范围查询。但这就是相似之处
的结尾：B树有着⾮常不同的设计理念。
* 我们前⾯看到的⽇志结构索引将数据库分解为可变⼤⼩的段，通常是⼏兆字节或更⼤的⼤⼩，并且总是
按顺序编写段。相⽐之下，B树将数据库分解成固定⼤⼩的块或⻚⾯，传统上⼤⼩为4KB（有时会更
⼤），并且⼀次只能读取或写⼊⼀个⻚⾯。这种设计更接近于底层硬件，因为磁盘也被安排在固定⼤⼩
的块中。
* ⼀个⻚⾯会被指定为B树的根；在索引中查找⼀个键时，就从这⾥开始。该⻚⾯包含⼏个键和对⼦⻚⾯
的引⽤。每个⼦⻚⾯负责⼀段连续范围的键，引⽤之间的键，指明了引⽤⼦⻚⾯的键范围。

##### 让B树更可靠
* B树的基本底层写操作是⽤新数据覆盖磁盘上的⻚⾯。假定覆盖不改变⻚⾯的位置;即，当⻚⾯被覆盖
时，对该⻚⾯的所有引⽤保持完整。这与⽇志结构索引（如LSM树）形成鲜明对⽐，后者只附加到⽂件
（并最终删除过时的⽂件），但从不修改⽂件。
* 可以考虑将硬盘上的⻚⾯覆盖为实际的硬件操作。在磁性硬盘驱动器上，这意味着将磁头移动到正确
的位置，等待旋转盘上的正确位置出现，然后⽤新的数据覆盖适当的扇区。在固态硬盘上，由于SSD必
须⼀次擦除和重写相当⼤的存储芯⽚块，所以会发⽣更复杂的事情。
* ⼀些操作需要覆盖⼏个不同的⻚⾯。例如，如果因为插⼊导致⻚⾯过度⽽拆分⻚⾯，则需要编写
已拆分的两个⻚⾯，并覆盖其⽗⻚⾯以更新对两个⼦⻚⾯的引⽤。这是⼀个危险的操作，因为如果数据
库在仅有⼀些⻚⾯被写⼊后崩溃，那么最终将导致⼀个损坏的索引（例如，可能有⼀个孤⼉⻚⾯不是任
何⽗项的⼦项）
* 为了使数据库对崩溃具有韧性，B树实现通常会带有⼀个额外的磁盘数据结构：预写式⽇志（WAL,
write-ahead-log）（也称为重做⽇志（redo log））。这是⼀个仅追加的⽂件，每个B树修改都可以
应⽤到树本身的⻚⾯上。当数据库在崩溃后恢复时，这个⽇志被⽤来使B树恢复到⼀致的状态
* 更新⻚⾯的⼀个额外的复杂情况是，如果多个线程要同时访问B树，则需要仔细的并发控制 —— 否则线
程可能会看到树处于不⼀致的状态。这通常通过使⽤锁存器（latches）（轻量级锁）保护树的数据结
构来完成。⽇志结构化的⽅法在这⽅⾯更简单，因为它们在后台进⾏所有的合并，⽽不会⼲扰传⼊的查
询，并且不时地将旧的分段原⼦交换为新的分段。
* ⼀些数据库（如LMDB）使⽤写时复制⽅案，⽽不是覆盖⻚⾯并维护WAL进⾏崩溃恢复。修
改的⻚⾯被写⼊到不同的位置，并且树中的⽗⻚⾯的新版本被创建，指向新的位置。这种⽅法对于
并发控制也很有⽤，我们将在“快照隔离和可重复读”中看到。
* 我们可以通过不存储整个键来节省⻚⾯空间，但可以缩⼩它的⼤⼩。特别是在树内部的⻚⾯上，键
只需要提供⾜够的信息来充当键范围之间的边界。在⻚⾯中包含更多的键允许树具有更⾼的分⽀因
⼦，因此更少的层次
* 通常，⻚⾯可以放置在磁盘上的任何位置；没有什么要求附近的键范围⻚⾯附近的磁盘上。如果查
询需要按照排序顺序扫描⼤部分关键字范围，那么每个⻚⾯的布局可能会⾮常不⽅便，因为每个读
取的⻚⾯都可能需要磁盘查找。因此，许多B树实现尝试布局树，使得叶⼦⻚⾯按顺序出现在磁盘
上。但是，随着树的增⻓，维持这个顺序是很困难的。相⽐之下，由于LSM树在合并过程中⼀次⼜
⼀次地重写存储的⼤部分，所以它们更容易使顺序键在磁盘上彼此靠近。
* 额外的指针已添加到树中。例如，每个叶⼦⻚⾯可以在左边和右边具有对其兄弟⻚⾯的引⽤，这允
许不跳回⽗⻚⾯就能顺序扫描。
* B树的变体如分形树借⽤⼀些⽇志结构的思想来减少磁盘寻道（⽽且它们与分形⽆关）。
##### B树优化
* ⼀些数据库（如LMDB）使⽤写时复制⽅案，⽽不是覆盖⻚⾯并维护WAL进⾏崩溃恢复。修
改的⻚⾯被写⼊到不同的位置，并且树中的⽗⻚⾯的新版本被创建，指向新的位置。这种⽅法对于
并发控制也很有⽤。
* 我们可以通过不存储整个键来节省⻚⾯空间，但可以缩⼩它的⼤⼩。特别是在树内部的⻚⾯上，键
只需要提供⾜够的信息来充当键范围之间的边界。在⻚⾯中包含更多的键允许树具有更⾼的分⽀因
⼦，因此更少的层次
* 通常，⻚⾯可以放置在磁盘上的任何位置；没有什么要求附近的键范围⻚⾯附近的磁盘上。如果查
询需要按照排序顺序扫描⼤部分关键字范围，那么每个⻚⾯的布局可能会⾮常不⽅便，因为每个读
取的⻚⾯都可能需要磁盘查找。因此，许多B树实现尝试布局树，使得叶⼦⻚⾯按顺序出现在磁盘
上。但是，随着树的增⻓，维持这个顺序是很困难的。相⽐之下，由于LSM树在合并过程中⼀次⼜
⼀次地重写存储的⼤部分，所以它们更容易使顺序键在磁盘上彼此靠近。
* 额外的指针已添加到树中。例如，每个叶⼦⻚⾯可以在左边和右边具有对其兄弟⻚⾯的引⽤，这允
许不跳回⽗⻚⾯就能顺序扫描。
* B树的变体如分形树借⽤⼀些⽇志结构的思想来减少磁盘寻道（⽽且它们与分形⽆关）。

#### ⽐较B树和LSM树
* 通常LSM树的写⼊速度更快，⽽B树的读取速度更快。 LSM树上的读取通常⽐较慢，因为它们必须在压缩的
不同阶段检查⼏个不同的数据结构和SSTables。
##### LSM树的优点
* B树索引必须⾄少两次写⼊每⼀段数据：⼀次写⼊预先写⼊⽇志，⼀次写⼊树⻚⾯本身（也许再次分
⻚）。即使在该⻚⾯中只有⼏个字节发⽣了变化，也需要⼀次编写整个⻚⾯的开销。有些存储引擎甚⾄
会覆盖同⼀个⻚⾯两次，以免在电源故障的情况下导致⻚⾯部分更新。
* 由于反复压缩和合并SSTables，⽇志结构索引也会重写数据。这种影响 —— 在数据库的⽣命周期中写
⼊数据库导致对磁盘的多次写⼊ —— 被称为写放⼤（write amplification）。需要特别关注的是固态
硬盘，固态硬盘在磨损之前只能覆写⼀段时间。
* 在写⼊繁重的应⽤程序中，性能瓶颈可能是数据库可以写⼊磁盘的速度。在这种情况下，写放⼤会导致
直接的性能代价：存储引擎写⼊磁盘的次数越多，可⽤磁盘带宽内的每秒写⼊次数越少。
* ⽽且，LSM树通常能够⽐B树⽀持更⾼的写⼊吞吐量，部分原因是它们有时具有较低的写放⼤（尽管这
取决于存储引擎配置和⼯作负载），部分是因为它们顺序地写⼊紧凑的SSTable⽂件⽽不是必须覆盖树
中的⼏个⻚⾯。这种差异在磁性硬盘驱动器上尤其重要，顺序写⼊⽐随机写⼊快得多。
* LSM树可以被压缩得更好，因此经常⽐B树在磁盘上产⽣更⼩的⽂件。 B树存储引擎会由于分割⽽留下
⼀些未使⽤的磁盘空间：当⻚⾯被拆分或某⾏不能放⼊现有⻚⾯时，⻚⾯中的某些空间仍未被使⽤。由
于LSM树不是⾯向⻚⾯的，并且定期重写SSTables以去除碎⽚，所以它们具有较低的存储开销，特别是
当使⽤平坦压缩时
* 在许多固态硬盘上，固件内部使⽤⽇志结构化算法，将随机写⼊转变为顺序写⼊底层存储芯⽚，因此存
储引擎写⼊模式的影响不太明显。但是，较低的写⼊放⼤率和减少的碎⽚对SSD仍然有利：更紧
凑地表示数据可在可⽤的I/O带宽内提供更多的读取和写⼊请求。
##### LSM树的缺点
* ⽇志结构存储的缺点是压缩过程有时会⼲扰正在进⾏的读写操作。尽管存储引擎尝试逐步执⾏压缩⽽不
影响并发访问，但是磁盘资源有限，所以很容易发⽣请求需要等待⽽磁盘完成昂贵的压缩操作。对吞吐
量和平均响应时间的影响通常很⼩，但是在更⾼百分⽐的情况下，对⽇志结构化存
储引擎的查询响应时间有时会相当⻓，⽽B树的⾏为则相对更具可预测性
* 压缩的另⼀个问题出现在⾼写⼊吞吐量：磁盘的有限写⼊带宽需要在初始写⼊（记录和刷新内存表到磁
盘）和在后台运⾏的压缩线程之间共享。写⼊空数据库时，可以使⽤全磁盘带宽进⾏初始写⼊，但数据
库越⼤，压缩所需的磁盘带宽就越多。
* 如果写⼊吞吐量很⾼，并且压缩没有仔细配置，压缩跟不上写⼊速率。在这种情况下，磁盘上未合并段
的数量不断增加，直到磁盘空间⽤完，读取速度也会减慢，因为它们需要检查更多段⽂件。通常情况
下，即使压缩⽆法跟上，基于SSTable的存储引擎也不会限制传⼊写⼊的速率，所以您需要进⾏明确的
监控来检测这种情况
* B树的⼀个优点是每个键只存在于索引中的⼀个位置，⽽⽇志结构化的存储引擎可能在不同的段中有相
同键的多个副本。这个⽅⾯使得B树在想要提供强⼤的事务语义的数据库中很有吸引⼒：在许多关系数
据库中，事务隔离是通过在键范围上使⽤锁来实现的，在B树索引中，这些锁可以直接连接到树。

#### 其他索引结构
* 到⽬前为⽌，我们只讨论了关键值索引，它们就像关系模型中的主键（primary key）索引。主键唯⼀
标识关系表中的⼀⾏，或⽂档数据库中的⼀个⽂档或图形数据库中的⼀个顶点。数据库中的其他记录可
以通过其主键（或ID）引⽤该⾏/⽂档/顶点，并且索引⽤于解析这样的引⽤。
* 有⼆级索引也很常⻅。在关系数据库中，您可以使⽤ CREATE INDEX 命令在同⼀个表上创建多个⼆级
索引，⽽且这些索引通常对于有效地执⾏联接⽽⾔⾄关重要。例如，在第2章中的图2-1中，很可能在
user_id 列上有⼀个⼆级索引，以便您可以在每个表中找到属于同⼀⽤户的所有⾏。
* ⼀个⼆级索引可以很容易地从⼀个键值索引构建。主要的不同是键不是唯⼀的。即可能有许多⾏（⽂
档，顶点）具有相同的键。这可以通过两种⽅式来解决：或者通过使索引中的每个值，成为匹配⾏标识
符的列表（如全⽂索引中的发布列表），或者通过向每个索引添加⾏标识符来使每个关键字唯⼀。⽆论
哪种⽅式，B树和⽇志结构索引都可以⽤作辅助索引。

##### 将值存储在索引中
* 索引中的关键字是查询搜索的内容，但是该值可以是以下两种情况之⼀：它可以是所讨论的实际⾏（⽂
档，顶点），也可以是对存储在别处的⾏的引⽤。在后⼀种情况下，⾏被存储的地⽅被称为堆⽂件
（heap file），并且存储的数据没有特定的顺序（它可以是仅附加的，或者可以跟踪被删除的⾏以便⽤
新数据覆盖它们后来）。堆⽂件⽅法很常⻅，因为它避免了在存在多个⼆级索引时复制数据：每个索引
只引⽤堆⽂件中的⼀个位置，实际的数据保存在⼀个地⽅。 在不更改键的情况下更新值时，堆⽂件⽅法
可以⾮常⾼效：只要新值不⼤于旧值，就可以覆盖该记录。如果新值更⼤，情况会更复杂，因为它可能
需要移到堆中有⾜够空间的新位置。在这种情况下，要么所有的索引都需要更新，以指向记录的新堆位
置，或者在旧堆位置留下⼀个转发指针。
在某些情况下，从索引到堆⽂件的额外跳跃对读取来说性能损失太⼤，因此可能希望将索引⾏直接存储
在索引中。这被称为聚集索引。例如，在MySQL的InnoDB存储引擎中，表的主键总是⼀个聚簇索引，
⼆级索引⽤主键（⽽不是堆⽂件中的位置）。在SQL Server中，可以为每个表指定⼀个聚簇索引
。
* 在聚集索引（clustered index）（在索引中存储所有⾏数据）和⾮聚集索引（nonclustered index）
（仅在索引中存储对数据的引⽤）之间的折衷被称为包含列的索引（index with included columns） 或覆盖索引（covering index），其存储表的⼀部分在索引内【33】。这允许通过单独使⽤索引来回
答⼀些查询（这种情况叫做：索引覆盖（cover）了查询）。
* 与任何类型的数据重复⼀样，聚簇和覆盖索引可以加快读取速度，但是它们需要额外的存储空间，并且
会增加写⼊开销。数据库还需要额外的努⼒来执⾏事务保证，因为应⽤程序不应该因为重复⽽导致不⼀
致。
##### 多列索引
* 最常⻅的多列索引被称为连接索引（concatenated index），它通过将⼀列的值追加到另⼀列后⾯，
简单地将多个字段组合成⼀个键（索引定义中指定了字段的连接顺序）。这就像⼀个⽼式的纸质电话
簿，它提供了⼀个从（姓，名）到电话号码的索引。由于排序顺序，索引可以⽤来查找所有具有特定姓
⽒的⼈，或所有具有特定姓-名组合的⼈。然⽽，如果你想找到所有具有特定名字的⼈，这个索引是没有
⽤的。
* 多维索引（multi-dimensional index）是⼀种查询多个列的更⼀般的⽅法，这对于地理空间数据尤为
重要。例如，餐厅搜索⽹站可能有⼀个数据库，其中包含每个餐厅的经度和纬度。当⽤户在地图上查看
餐馆时，⽹站需要搜索⽤户正在查看的矩形地图区域内的所有餐馆。这需要⼀个⼆维范围查询。
  * SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079
 AND longitude > -0.1162 AND longitude < -0.1004;
* ⼀个标准的B树或者LSM树索引不能够⾼效地响应这种查询：它可以返回⼀个纬度范围内的所有餐馆
（但经度可能是任意值），或者返回在同⼀个经度范围内的所有餐馆（但纬度可能是北极和南极之间的
任意地⽅），但不能同时满⾜。
*  ⼀种选择是使⽤空间填充曲线将⼆维位置转换为单个数字，然后使⽤常规B树索引。更普遍的
是，使⽤特殊化的空间索引，例如R树。例如，PostGIS使⽤PostgreSQL的通⽤Gist⼯具将地理
空间索引实现为R树。
* 多维索引不仅可以⽤于地理位置。例如，在电⼦商务⽹站上可以使⽤维度（红⾊，
绿⾊，蓝⾊）上的三维索引来搜索特定颜⾊范围内的产品，也可以在天⽓观测数据库中搜索⼆维（⽇
期，温度）的指数，以便有效地搜索2013年的温度在25⾄30°C之间的所有观测资料。使⽤⼀维索引，
你将不得不扫描2013年的所有记录（不管温度如何），然后通过温度进⾏过滤，反之亦然。 ⼆维索引
可以同时通过时间戳和温度来收窄数据集。这个技术被HyperDex使⽤。
##### 全⽂搜索和模糊索引
* 到⽬前为⽌所讨论的所有索引都假定您有确切的数据，并允许您查询键的确切值或具有排序顺序的键的
值范围。他们不允许你做的是搜索类似的键，如拼写错误的单词。这种模糊的查询需要不同的技术。
* 全⽂搜索引擎通常允许搜索⼀个单词以扩展为包括该单词的同义词，忽略单词的语法变体，并且
搜索在相同⽂档中彼此靠近的单词的出现，并且⽀持各种其他功能取决于⽂本的语⾔分析。为了处理⽂
档或查询中的拼写错误，Lucene能够在⼀定的编辑距离内搜索⽂本。
* 正如“在SSTables中创建LSM树”中所提到的，Lucene为其词典使⽤了⼀个类似于SSTable的结构。这个
结构需要⼀个⼩的内存索引，告诉查询在排序⽂件中哪个偏移量需要查找关键字。在LevelDB中，这个
内存中的索引是⼀些键的稀疏集合，但在Lucene中，内存中的索引是键中字符的有限状态⾃动机，类似
于trie。这个⾃动机可以转换成Levenshtein⾃动机，它⽀持在给定的编辑距离内有效地搜索单
词。
##### 在内存中存储⼀切
* 内存数据库重新启动时，需要从磁盘或通过⽹络从副本重新加载其状态（除⾮使⽤特殊的硬件）。尽管
写⼊磁盘，它仍然是⼀个内存数据库，因为磁盘仅⽤作耐久性附加⽇志，读取完全由内存提供。写⼊磁
盘也具有操作优势：磁盘上的⽂件可以很容易地由外部实⽤程序进⾏备份，检查和分析。
* 反直觉的是，内存数据库的性能优势并不是因为它们不需要从磁盘读取的事实。即使是基于磁盘的存储
引擎也可能永远不需要从磁盘读取，因为操作系统缓存最近在内存中使⽤了磁盘块。相反，它们更快的
原因在于省去了将内存数据结构编码为磁盘数据结构的开销。
* 除了性能，内存数据库的另⼀个有趣的领域是提供难以⽤基于磁盘的索引实现的数据模型。例如，
Redis为各种数据结构（如优先级队列和集合）提供了类似数据库的接⼝。因为它将所有数据保存在内
存中，所以它的实现相对简单。

#### 事务处理还是分析？
* 在业务数据处理的早期，对数据库的写⼊通常对应于正在进⾏的商业交易：进⾏销售，向供应商下订
单，⽀付员⼯⼯资等等。随着数据库扩展到那些没有不涉及钱易⼿，术语交易仍然卡住，指的是形成⼀
个逻辑单元的⼀组读写。 事务不⼀定具有ACID（原⼦性，⼀致性，隔离性和持久性）属性。事务处理
只是意味着允许客户端进⾏低延迟读取和写⼊ —— ⽽不是批量处理作业，⽽这些作业只能定期运⾏
（例如每天⼀次）。我们在第7章中讨论ACID属性，在第10章中讨论批处理。
* 即使数据库开始被⽤于许多不同类型的博客⽂章，游戏中的动作，地址簿中的联系⼈等等，基本访问模
式仍然类似于处理业务事务。应⽤程序通常使⽤索引通过某个键查找少量记录。根据⽤户的输⼊插⼊或
更新记录。由于这些应⽤程序是交互式的，因此访问模式被称为在线事务处理（OLTP, OnLine
Transaction Processing）。
* 但是，数据库也开始越来越多地⽤于数据分析，这些数据分析具有⾮常不同的访问模式。通常，分析查
询需要扫描⼤量记录，每个记录只读取⼏列，并计算汇总统计信息（如计数，总和或平均值），⽽不是
将原始数据返回给⽤户。例如，如果您的数据是⼀个销售交易表，那么分析查询可能是
  * ⼀⽉份我们每个商店的总收⼊是多少？
  * 我们在最近的推⼴活动中销售多少⾹蕉？
  * 哪种品牌的婴⼉⻝品最常与X品牌的尿布⼀起购买？
* 这些查询通常由业务分析师编写，并提供给帮助公司管理层做出更好决策（商业智能）的报告。为了区
分这种使⽤数据库的事务处理模式，它被称为在线分析处理（OLAP, OnLine Analytice
Processing）。
* OLTP vs OLAP
  * 主要读取模式：查询少量，按键读取 vs 在大批量记录上聚合
  * 主要写入模式：随机访问，写入要求低延时 vs 批量导入，事件流
  * 主要用户：终端用户，通过Web应用 vs 内部数据分析师，决策支持
  * 处理的数据： 数据的最新状态 vs 随时间推移的历史事件
  * 数据集尺寸：GB ~ TB vs TB ~ PB

#### 数据仓库
* ⼀个企业可能有⼏⼗个不同的交易处理系统：系统为⾯向客户的⽹站提供动⼒，控制实体商店的销售点
（checkout）系统，跟踪仓库中的库存，规划⻋辆路线，管理供应商，管理员⼯等。这些系统中的每
⼀个都是复杂的，需要⼀个⼈员去维护，所以系统最终都是⾃动运⾏的。
* 这些OLTP系统通常具有⾼度的可⽤性，并以低延迟处理事务，因为这些系统往往对业务运作⾄关重
要。因此数据库管理员密切关注他们的OLTP数据库他们通常不愿意让业务分析⼈员在OLTP数据库上运
⾏临时分析查询，因为这些查询通常很昂贵，扫描⼤部分数据集，这会损害同时执⾏的事务的性能。
* 相⽐之下，数据仓库是⼀个独⽴的数据库，分析⼈员可以查询他们⼼中的内容，⽽不影响OLTP操作
。数据仓库包含公司所有各种OLTP系统中的只读数据副本。从OLTP数据库中提取数据（使⽤定
期的数据转储或连续的更新流），转换成适合分析的模式，清理并加载到数据仓库中。将数据存⼊仓库
的过程称为“抽取-转换-加载（ETL）”，如图3-8所示。
* ⼏乎所有的⼤型企业都有数据仓库，但在⼩型企业中⼏乎闻所未闻。这可能是因为⼤多数⼩公司没有这
么多不同的OLTP系统，⼤多数⼩公司只有少量的数据 —— 可以在传统的SQL数据库中查询，甚⾄可以
在电⼦表格中分析。在⼀家⼤公司⾥，要做⼀些在⼀家⼩公司很简单的事情，需要很多繁重的⼯作。
* 使⽤单独的数据仓库，⽽不是直接查询OLTP系统进⾏分析的⼀⼤优势是数据仓库可针对分析访问模式
进⾏优化。事实证明，本章前半部分讨论的索引算法对于OLTP来说⼯作得很好，但对于回答分析查询
并不是很好。在本章的其余部分中，我们将看看为分析⽽优化的存储引擎。

##### OLTP数据库和数据仓库之间的分歧
* 数据仓库的数据模型通常是关系型的，因为SQL通常很适合分析查询。有许多图形数据分析⼯具可以⽣
成SQL查询，可视化结果，并允许分析⼈员探索数据（通过下钻，切⽚和切块等操作）。
* 表⾯上，⼀个数据仓库和⼀个关系OLTP数据库看起来很相似，因为它们都有⼀个SQL查询接⼝。然⽽，
系统的内部看起来可能完全不同，因为它们针对⾮常不同的查询模式进⾏了优化。现在许多数据库供应
商都将重点放在⽀持事务处理或分析⼯作负载上，⽽不是两者都⽀持。
* ⼀些数据库（例如Microsoft SQL Server和SAP HANA）⽀持在同⼀产品中进⾏事务处理和数据仓库。
但是，它们正在⽇益成为两个独⽴的存储和查询引擎，这些引擎正好可以通过⼀个通⽤的SQL接⼝访问。
* Teradata，Vertica，SAP HANA和ParAccel等数据仓库供应商通常使⽤昂贵的商业许可证销售他们的系
统。 Amazon RedShift是ParAccel的托管版本。最近，⼤量的开源SQL-on-Hadoop项⽬已经出现，它
们还很年轻，但是正在与商业数据仓库系统竞争。这些包括Apache Hive，Spark SQL，Cloudera
Impala，Facebook Presto，Apache Tajo和Apache Drill 【52,53】。其中⼀些是基于⾕歌的Dremel
的想法。

#### 星型和雪花型：分析的模式
* 根据应⽤程序的需要，在事务处理领域中使⽤了⼤量不同的数据模型。另⼀⽅
⾯，在分析中，数据模型的多样性则少得多。许多数据仓库都以相当公式化的⽅式使⽤，被称为星型模
式（也称为维度建模）。
* 通常情况下，事实被视为单独的事件，因为这样可以在以后分析中获得最⼤的灵活性。但是，这意味着
事实表可以变得⾮常⼤。
* 事实表中的⼀些列是属性，例如产品销售的价格和从供应商那⾥购买的成本（允许计算利润余额）。事
实表中的其他列是对其他表（称为维表）的外键引⽤。由于事实表中的每⼀⾏都表示⼀个事件，因此这
些维度代表事件的发⽣地点，时间，⽅式和原因。
* 例如⼀个维度是已售出的产品。 dim_product 表中的每⼀⾏代表⼀种待售产品，
包括库存单位（SKU），说明，品牌名称，类别，脂肪含量，包装尺⼨等。 fact_sales 表中的每⼀⾏
都使⽤外部表明在特定交易中销售了哪些产品。 （为了简单起⻅，如果客户⼀次购买⼏种不同的产品，
则它们在事实表中被表示为单独的⾏）。
* 即使⽇期和时间通常使⽤维度表来表示，因为这允许对⽇期（诸如公共假期）的附加信息进⾏编码，从
⽽允许查询区分假期和⾮假期的销售。
* “星型模式”这个名字来源于这样⼀个事实，即当表关系可视化时，事实表在中间，由维表包围；与这些
表的连接就像星星的光芒。
* 这个模板的变体被称为雪花模式，其中尺⼨被进⼀步分解为⼦尺⼨。例如，品牌和产品类别可能有单独
的表格，并且 dim_product 表格中的每⼀⾏都可以将品牌和类别作为外键引⽤，⽽不是将它们作为字
符串存储在 dim_product 表格中。雪花模式⽐星形模式更规范化，但是星形模式通常是⾸选，因为分
析师使⽤它更简单。
* 在典型的数据仓库中，表格通常⾮常宽泛：事实表格通常有100列以上，有时甚⾄有数百列。维
度表也可以是⾮常宽的，因为它们包括可能与分析相关的所有元数据——例如， dim_store 表可以包
括在每个商店提供哪些服务的细节，它是否具有店内⾯包房，⽅形镜头，商店第⼀次开幕的⽇期，最后
⼀次改造的时间，离最近的⾼速公路的距离等等。
* 这个模板的变体被称为雪花模式，其中尺⼨被进⼀步分解为⼦尺⼨。例如，品牌和产品类别可能有单独
的表格，并且 dim_product 表格中的每⼀⾏都可以将品牌和类别作为外键引⽤，⽽不是将它们作为字
符串存储在 dim_product 表格中。雪花模式⽐星形模式更规范化，但是星形模式通常是⾸选，因为分
析师使⽤它更简单。
* 在典型的数据仓库中，表格通常⾮常宽泛：事实表格通常有100列以上，有时甚⾄有数百列【51】。维
度表也可以是⾮常宽的，因为它们包括可能与分析相关的所有元数据——例如， dim_store 表可以包
括在每个商店提供哪些服务的细节，它是否具有店内⾯包房，⽅形镜头，商店第⼀次开幕的⽇期，最后
⼀次改造的时间，离最近的⾼速公路的距离等等。

#### 列存储
* 如果事实表中有万亿⾏和数PB的数据，那么⾼效地存储和查询它们就成为⼀个具有挑战性的问题。维度
表通常要⼩得多（数百万⾏），所以在本节中我们将主要关注事实的存储。
* 尽管事实表通常超过100列，但典型的数据仓库查询⼀次只能访问4个或5个查询（ “ SELECT * ” 查询
很少⽤于分析）。以例3-1中的查询为例：它访问了⼤量的⾏（在2013⽇历年中每次都有⼈购买
⽔果或糖果），但只需访问 fact_sales 表的三列： date_key, product_sk, quantity 。查询忽略
所有其他列。
* 在⼤多数OLTP数据库中，存储都是以⾯向⾏的⽅式进⾏布局的：表格的⼀⾏中的所有值都相邻存储。
⽂档数据库是相似的：整个⽂档通常存储为⼀个连续的字节序列。
* 为了处理像例3-1这样的查询，您可能在 fact_sales.date_key ， fact_sales.product_sk 上有索
引，它们告诉存储引擎在哪⾥查找特定⽇期或特定产品的所有销售情况。但是，⾯向⾏的存储引擎仍然
需要将所有这些⾏（每个包含超过100个属性）从磁盘加载到内存中，解析它们，并过滤掉那些不符合
要求的条件。这可能需要很⻓时间。
* ⾯向列的存储背后的想法很简单：不要将所有来⾃⼀⾏的值存储在⼀起，⽽是将来⾃每⼀列的所有值存
储在⼀起。如果每个列存储在⼀个单独的⽂件中，查询只需要读取和解析查询中使⽤的那些列，这可以
节省⼤量的⼯作。

##### 列压缩
* 除了仅从磁盘加载查询所需的列以外，我们还可以通过压缩数据来进⼀步降低对磁盘吞吐量的需求。幸
运的是，⾯向列的存储通常很适合压缩。

##### 列存储中的排序顺序
* 在列存储中，存储⾏的顺序并不⼀定很重要。按插⼊顺序存储它们是最简单的，因为插⼊⼀个新⾏就意
味着附加到每个列⽂件。但是，我们可以选择强制执⾏⼀个命令，就像我们之前对SSTables所做的那
样，并将其⽤作索引机制。
* 在⼀个⾯向列的存储中有多个排序顺序有点类似于在⼀个⾯向⾏的存储中有多个⼆级索引。但最⼤的区
别在于⾯向⾏的存储将每⼀⾏保存在⼀个地⽅（在堆⽂件或聚簇索引中），⼆级索引只包含指向匹配⾏
的指针。在列存储中，通常在其他地⽅没有任何指向数据的指针，只有包含值的列。
#### 写⼊列存储
* 这些优化在数据仓库中是有意义的，因为⼤多数负载由分析⼈员运⾏的⼤型只读查询组成。⾯向列的存
储，压缩和排序都有助于更快地读取这些查询。然⽽，他们有写更加困难的缺点。
* 使⽤B树的更新就地⽅法对于压缩的列是不可能的。如果你想在排序表的中间插⼊⼀⾏，你很可能不得
不重写所有的列⽂件。由于⾏由列中的位置标识，因此插⼊必须始终更新所有列。
* 幸运的是，本章前⾯已经看到了⼀个很好的解决⽅案：LSM树。所有的写操作⾸先进⼊⼀个内存中的存
储，在这⾥它们被添加到⼀个已排序的结构中，并准备写⼊磁盘。内存中的存储是⾯向⾏还是列的，这
并不重要。当已经积累了⾜够的写⼊数据时，它们将与磁盘上的列⽂件合并，并批量写⼊新⽂件。这基
本上是Vertica所做的。
* 查询需要检查磁盘上的列数据和最近在内存中的写⼊，并将两者结合起来。但是，查询优化器隐藏了⽤
户的这个区别。从分析师的⻆度来看，通过插⼊，更新或删除操作进⾏修改的数据会⽴即反映在后续查
询中。
##### 聚合：数据⽴⽅体和物化视图
* 并不是每个数据仓库都必定是⼀个列存储：传统的⾯向⾏的数据库和其他⼀些架构也被使⽤。然⽽，对
于专⻔的分析查询，列式存储可以显着加快，所以它正在迅速普及。
* 数据仓库的另⼀个值得⼀提的是物化汇总。如前所述，数据仓库查询通常涉及⼀个聚合函数，如SQL中 的COUNT，SUM，AVG，MIN或MAX。如果相同的聚合被许多不同的查询使⽤，那么每次都可以通过
原始数据来处理。为什么不缓存⼀些查询使⽤最频繁的计数或总和？
* 创建这种缓存的⼀种⽅式是物化视图。在关系数据模型中，它通常被定义为⼀个标准（虚拟）视图：⼀
个类似于表的对象，其内容是⼀些查询的结果。不同的是，物化视图是查询结果的实际副本，写⼊磁
盘，⽽虚拟视图只是写⼊查询的捷径。从虚拟视图读取时，SQL引擎会将其展开到视图的底层查询中，
然后处理展开的查询。
* 当底层数据发⽣变化时，物化视图需要更新，因为它是数据的⾮规范化副本。数据库可以⾃动完成，但
是这样的更新使得写⼊成本更⾼，这就是在OLTP数据库中不经常使⽤物化视图的原因。在读取繁重的
数据仓库中，它们可能更有意义（不管它们是否实际上改善了读取性能取决于个别情况。
* 物化视图的常⻅特例称为数据⽴⽅体或OLAP⽴⽅。它是按不同维度分组的聚合⽹格。图3-12显
示了⼀个例⼦。

#### 本章⼩结
* 在本章中，我们试图深⼊了解数据库如何处理存储和检索。将数据存储在数据库中会发⽣什么，以及稍
后再次查询数据时数据库会做什么？
* 在⾼层次上，我们看到存储引擎分为两⼤类：优化事务处理（OLTP）和优化分析（OLAP）的类别。这
些⽤例的访问模式之间有很⼤的区别：
  * OLTP系统通常⾯向⽤户，这意味着他们可能会看到⼤量的请求。为了处理负载，应⽤程序通常只
触及每个查询中的少量记录。应⽤程序使⽤某种键来请求记录，存储引擎使⽤索引来查找所请求的
键的数据。磁盘寻道时间往往是这⾥的瓶颈。
  * 数据仓库和类似的分析系统不太知名，因为它们主要由业务分析⼈员使⽤，⽽不是由最终⽤户使
⽤。它们处理⽐OLTP系统少得多的查询量，但是每个查询通常要求很⾼，需要在短时间内扫描数
百万条记录。磁盘带宽（不是查找时间）往往是瓶颈，列式存储是这种⼯作负载越来越流⾏的解决
⽅案。

* OLTP两⼤主流学派的存储引擎
  * ⽇志结构学派: 只允许附加到⽂件和删除过时的⽂件，但不会更新已经写⼊的⽂件。 Bitcask，SSTables，LSM树，LevelDB，Cassandra，HBase，Lucene等都属于这个组。
  * 就地更新学派: 将磁盘视为⼀组可以覆盖的固定⼤⼩的⻚⾯。 B树是这种哲学的最⼤的例⼦，被⽤在所有主要的关系数据库中，还有许多⾮关系数据库。
* ⽇志结构的存储引擎是相对较新的发展。他们的主要想法是，他们系统地将随机访问写⼊顺序写⼊磁
盘，由于硬盘驱动器和固态硬盘的性能特点，可以实现更⾼的写⼊吞吐量。在完成OLTP⽅⾯，我们通
过⼀些更复杂的索引结构和为保留所有数据⽽优化的数据库做了⼀个简短的介绍。
* 典型数据仓库的⾼级架构。这⼀背景说明了为什么分析⼯作负
载与OLTP差别很⼤：当您的查询需要在⼤量⾏中顺序扫描时，索引的相关性就会降低很多。相反，⾮
常紧凑地编码数据变得⾮常重要，以最⼤限度地减少查询需要从磁盘读取的数据量。我们讨论了列式存
储如何帮助实现这⼀⽬标。
* 作为⼀名应⽤程序开发⼈员，如果您掌握了有关存储引擎内部的知识，那么您就能更好地了解哪种⼯具
最适合您的特定应⽤程序。如果您需要调整数据库的调整参数，这种理解可以让您设想⼀个更⾼或更低
的值可能会产⽣什么效果。

##### ⼏个不同的排序顺序

#### 写⼊列存储



## 第四章 编码与演化
* 大多数情况下，更改app功能时，其存储的数据也需要更改。但面临很多挑战
  * 服务端app需要滚动升级
  * 客户端app需要客户配合升级，但是往往客户不会第一时间更新
* 需要保持向前后兼容

### 数据编码格式
* 程序中常见的两种数据表示形式
  * 内存中，数据保存在对象、结构体、列表、数组、哈希表和树等数据结构中。它们针对CPU的高效访问和操作进行了优化。(通常使用指针)
  * 将数据写入文件或通过网络发送时，需要序列化成字节序列，例如JSON。由于指针对其他进程无意义，所以字节序列和内存中结构是不一样的。

#### 语言的特定格式
* Java Serializable, Python pickle等
* 问题
  * 编码(序列化)跨语言访问非常困难
  * 解码(反序列化)过程有安全隐患
  * 向前后兼容容易被忽略
  * 性能问题

#### JSON、XML与二进制变体
* JSON，XML和CSV都是文本格式，具有不错的可读性，但是也有一定问题
  * 数字编码问题，XML和CSV中，数字和数字字符串无法区分。JSON可以区分，但不能区分小数和整数。并且它们处理大数字时也有问题(2^53以上的数在IEEE 754双精度浮点数中不能精确表示)。
  * JSON和XML不支持二进制字符串。它们有模式支持，但是学习起来复杂。
  * CSV没有模式支持。

#### 二进制编码
* 当数据量达到TB级别时，数据格式的选择就会产生很大影响。
* JSON比XML简约，但比起二进制格式，还是占用了很大空间。
  * BSON, BJSON, MessagePack, UBJSON, BISON, Smile等

#### Thrift, Protocol Buffers, Avro
* 相同原理的两种二进制代码库

### 数据流模式
* 最常见的进程间数据流动方式
  * 数据库
  * 服务调用
  * 异步消息
#### 基于数据库的数据流
* 向前后兼容性是必要的
###### 不同时间写入不同值
* 数据库内的数据可能是多年前的，即使应用版本经过了很多次更新。数据比代码更长久
* 将数据重写为新模式是可行的，但是代价很大。
* 大多数关系型数据库允许进行简单的模式更改，例如添加具有默认值空的新列， 而不重写现有数据。

##### 归档存储
* 经常为数据库创建快照的场景，例如备份或加载到数据仓库。这种情况下通常将数据转存为最新的模式编码。

### 基于服务的数据流：REST和RPC
* 在某些方面，服务类似于数据库，它们通常允许客户端提交和查询数据。
* SOA和Microservice的一个关键设计目标是，通过使服务可独立部署演化，让应用程序更易更改和维护。

#### 网络服务
* 客户端到服务端
* 服务端到服务端
* 公共API
* REST
  * 基于HTTP原则的设计理念，强调简单的数据格式，使用URL来标识资源，使用HTTP功能进行缓存控制、身份验证和内容类型协商
* SOAP
  * 基于XML，目的是独立于HTTP，避免使用大多数HTTP功能。

#### RPC的问题
* 网络请求不可预测，并且网络问题很常见
* 由于超时，无法知道究竟发生了什么
* 如果重试失败的网络请求，可能会发生请求实际上已经完成，只是响应丢失的情况。会导致重试操作被执行多次，造成重复数据。
* 性能问题
* 需要序列化和反序列化
* 不同语言之间的通信会有问题

#### RPC发展方向

#### RPC的数据编码和演化

### 基于消息传递的数据流
* 通过消息中间件发送异步消息。
* 优点
  * 如果接收方不可用或过载，消息代理可以充当缓冲区，从而提高系统可用性
  * 可以自动将消息重新发送到崩溃的进程，从而防止消息丢失
  * 避免了发送方需要知道对方的地址
  * 支持广播模式
  * 逻辑上将发送方和接收方解耦

#### 消息代理
#### 分布式Actor框架
* 用于单个进程中并非的编程模型。每个Actor代表一个客户端或实体，它可能具有某些本地状态，并且它通过发送和接收异步消息与其他Actor通信。
* Akka: 使用Java内置的序列化，不提供向前后兼容的能力。但是可以使用Protocol Buffer来获得滚动升级的能力
* Orleans
* Erlang OTP

### 小结


# 第二部分 分布式数据
* 分布在多台机器上的数据，对于可扩展性是必须的，同时带来了许多独特挑战
  * 可扩展性
  * 容错/高可用性
  * 降低地理延时
## 扩展至更高的负荷
* 垂直扩展：使用更强大的硬件
  * 成本增长的速度快于线性增长，双倍的性能需要不止双倍的成本
  * 提供有限的容错能力，但是受到地理位置限制
  * 共享磁盘架构：多台独立CPU和内存的机器共享一个磁盘。用于一些数据仓库，但是竞争和锁定的开销限制了共享磁盘的可扩展性
### 无共享架构(水平扩展)
* 多节点，每个节点只使用自己独立的硬件，节点间的所有协调通过软件和网络实现。
* 无需特殊的硬件，使用云平台可以轻松实现

## 复制 vs 分区
#### 复制（Replication）
* 在多个节点上保持数据的相同副本，提供了冗余。可以提升可用性和性能。
#### 分区(partitioning/sharding)
* 将一个大型数据拆分成较小的子集，从而不同的分区可以指派给不同的节点。
* 复制和分区是不同的机制，但它们经常同时使用。

## 第五章 复制
### 领导者与追随者
* 数据库的写操作需要传播到所有副本上，否则副本就会包含不一样的数据。
  * 基于领导者的复制
    * 写入请求都发给领导者，领导者将新数据写入本地存储
    * 领导者写入本地存储时，将数据变更发送给所有的追随者(replication log/change stream)。每个跟随着从领导者拉取日志，并更新相应的本地数据库，按照领导者处理的相同顺序写入
    * 客户端读操作可以向领导者或追随者查询。
### 同步复制与异步复制
* 同步：从库保证与主库一致的最新数据副本。如果主库突然失效，可以确保这些数据仍然能在从库上找到。但是如果同步从库的过程没有相应，主库就无法处理写入操作。因此，将所有从库都设为同步时不实际的，因为任何一个节点故障都会导致整个系统停滞。实际上，如果数据库启动同步，通常意味着只有一个跟随者是同步的，其他则是异步的。这种配置也被称为半同步。
* 通常情况下，基于领导者的复制都配置为完全异步，这样如果主库失效且不可恢复，任何尚未复制给从库的写入都会丢失。这可能会导致即使已经向客户端确认成功，写入也不能保证持久。优点是即使所有的从库都落后了，主库也可以继续处理写入。

### 设置新从库
* 添加一个全新的从库的步骤(通常不需要停机)
  * 在某个时刻获取主库的一致性快照(如果可能)，而不必锁定整个数据库。
  * 将快照复制到新的从库节点。
  * 从库连接到主库，并拉取快照之后所有的数据变更。
  * 当从库处理完快照之后挤压的数据变更，它就可以继续处理主库产生的数据变化了。

### 处理节点宕机
#### 从库失效：追赶恢复
* 从库从日志中可以知道发送故障前最后一个事务，然后连接到主库，同步断开之后的所有数据变更。
#### 主库失效：故障切换(failover)
* 一个从库需要被提升为新的主库，并且重新配置客户端，以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。
* 自动故障切换的步骤
  * 确定主库失效，但并没有万无一失的方法来检查，通常只是使用简单的超时机制。
  * 选择一个新的主库。可以通过选举或仲裁节点来指定。
  * 重新配置系统以启动新的主库，同时老的主库恢复后称为从库。

### 复制日志的实现
#### 基于语句的复制
* 主库记录下所有的写入请求(例如SQL语句)，并将该语句日志发送给从库。
  * 非确定性函数可能会产生不同的值，如NOW()、RAND()方法。
  * 自增列或依赖本地现有数据的表需要在副本上用完全相同的顺序执行它们。当有多个并发事务时，这可能成为一个限制。
  * 有副作用(触发器，存储过程，用户定义的函数)的语句可能在每个副本上产生不同的副作用，触发副作用时确定的。

#### 传输预写式日志(Write Ahead Log, WAL)
* ⽇志都是包含所有数据库写⼊的仅追加字节序列。可以使⽤完全相同的⽇志在另⼀
个节点上构建副本：除了将⽇志写⼊磁盘之外，主库还可以通过⽹络将其发送给其从库。
当从库应⽤这个⽇志时，它会建⽴和主库⼀模⼀样数据结构的副本。

#### 逻辑⽇志复制（基于⾏）
* 复制和存储引擎使⽤不同的⽇志格式，这样可以使复制⽇志从存储引擎内部分离出来。
这种复制⽇志被称为逻辑⽇志，以将其与存储引擎的（物理）数据表示区分开来。
  * 对于插⼊的⾏，⽇志包含所有列的新值
  * 对于删除的⾏，⽇志包含⾜够的信息来唯⼀标识已删除的⾏。通常是主键，但是如果表上没有主
键，则需要记录所有列的旧值。
  * 对于更新的⾏，⽇志包含⾜够的信息来唯⼀标识更新的⾏，以及所有列的新值（或⾄少所有已更改
的列的新值）。

#### 基于触发器的复制
* 触发器允许您注册在数据库系统中发⽣数据更改（写⼊事务）时⾃动执⾏的⾃定义应⽤程序代码。触发
器有机会将更改记录到⼀个单ᇿ的表中，使⽤外部程序读取这个表，再加上任何业务逻辑处理，会后将
数据变更复制到另⼀个系统去。例如，Databus for Oracle 【20】和Bucardo for Postgres 【21】就是
这样⼯作的。

### 复制延迟问题
* 当应⽤程序从异步从库读取时，如果从库落后，它可能会看到过时的信息。这会导致数据库
中出现明显的不⼀致：同时对主库和从库执⾏相同的查询，可能得到不同的结果，因为并⾮所有的写⼊
都反映在从库中。这种不⼀致只是⼀个暂时的状态——如果停⽌写⼊数据库并等待⼀段时间，从库最终
会赶上并与主库保持⼀致。出于这个原因，这种效应被称为最终⼀致性（eventually
consistency）

#### 读⼰之写
* 如果用户重新加载⻚⾯，他们总会看到
他们⾃⼰提交的任何更新。它不会对其他⽤户的写⼊做出承诺：其他⽤户的更新可能稍等才会看到。它
保证⽤户⾃⼰的输⼊已被正确保存。
  * 从主库读取⽤户⾃⼰的档案，在从库读取其他⽤户的档案。
  * 如果应⽤中的⼤部分内容都可能被⽤户编辑，那这种⽅法就没⽤了，因为⼤部分内容都必须从主库
读取（扩容读就没效果了）。在这种情况下可以使⽤其他标准来决定是否从主库读取。例如可以跟
踪上次更新的时间，在上次更新后的⼀分钟内，从主库读。还可以监控从库的复制延迟，防⽌任向
任何滞后超过⼀分钟到底从库发出查询。
  * 客户端可以记住最近⼀次写⼊的时间戳，系统需要确保从库为该⽤户提供任何查询时，该时间戳前
的变更都已经传播到了本从库中。如果当前从库不够新，则可以从另⼀个从库读，或者等待从库追
赶上来。
  * 如果您的副本分布在多个数据中⼼（出于可⽤性⽬的与⽤户尽量在地理上接近），则会增加复杂
性。任何需要由领导者提供服务的请求都必须路由到包含主库的数据中⼼

#### 单调读
* 实现单调读取的⼀种⽅式是确保每个⽤户总是从同⼀个副本进⾏读取（不同的⽤户可以从不同的副本读
取）。例如，可以基于⽤户ID的散列来选择副本，⽽不是随机选择副本。但是，如果该副本失败，⽤户
的查询将需要重新路由到另⼀个副本。

#### ⼀致前缀读
* 如果⼀系列写⼊按某个顺序发⽣，那么任何⼈读取这些写⼊时，也会看⻅它们以同样的顺序出现。
* 这是分区（partitioned）（分⽚（sharded））数据库中的⼀个特殊问题，将在第6章中讨论。如果数
据库总是以相同的顺序应⽤写⼊，则读取总是会看到⼀致的前缀，所以这种异常不会发⽣。但是在许多
分布式数据库中，不同的分区ᇿ⽴运⾏，因此不存在全局写⼊顺序：当⽤户从数据库中读取数据时，可
能会看到数据库的某些部分处于较旧的状态，⽽某些处于较新的状态。
* ⼀种解决⽅案是，确保任何因果相关的写⼊都写⼊相同的分区。对于某些⽆法⾼效完成这种操作的应
⽤，还有⼀些显式跟踪因果依赖关系的算法，本书将在“关系与并发”⼀节中返回这个主题。

#### 复制延迟的解决⽅案
* 在使⽤最终⼀致的系统时，如果复制延迟增加到⼏分钟甚⾄⼏⼩时，则应该考虑应⽤程序的⾏为。如果
答案是“没问题”，那很好。但如果结果对于⽤户来说是不好体验，那么设计系统来提供更强的保证是很
重要的，例如写后读。明明是异步复制却假设复制是同步的，这是很多麻烦的根源

### 多主复制
#### 多主复制的应⽤场景
##### 运维多个数据中⼼
* 多领导者配置中可以在每个数据中⼼都有主库。在每个数据中⼼内使⽤
常规的主从复制；在数据中⼼之间，每个数据中⼼的主库都会将其更改复制到其他数据中⼼的主库中。
  * 更高的性能
  * 容忍数据中心停机
  * 容忍网络问题
##### 需要离线操作的客户端
* 每个设备都有⼀个充当领导者的本地数据库（它接受写请求），并且在所有设备上的⽇
历副本之间同步时，存在异步的多主复制过程。复制延迟可能是⼏⼩时甚⾄⼏天，具体取决于何时可以
访问互联⽹。

##### 协同编辑
* 当⼀个⽤户编辑⽂档时，所做
的更改将⽴即应⽤到其本地副本（Web浏览器或客户端应⽤程序中的⽂档状态），并异步复制到服务器
和编辑同⼀⽂档的任何其他⽤户。
* 如果要保证不会发⽣编辑冲突，则应⽤程序必须先取得⽂档的锁定，然后⽤户才能对其进⾏编辑。如果
另⼀个⽤户想要编辑同⼀个⽂档，他们⾸先必须等到第⼀个⽤户提交修改并释放锁定。这种协作模式相
当于在领导者上进⾏交易的单领导者复制。
* 但是，为了加速协作，您可能希望将更改的单位设置得⾮常⼩（例如，⼀个按键），并避免锁定。这种
⽅法允许多个⽤户同时进⾏编辑，但同时也带来了多领导者复制的所有挑战，包括需要解决冲突

#### 处理写⼊冲突

##### 同步与异步冲突检测
##### 避免冲突
##### 收敛⾄⼀致的状态
##### ⾃定义冲突解决逻辑
* 写时执⾏
* 读时执⾏

#### 多主复制拓扑
* Circular Topology
* Star Topology
* All-to-all Topology
  * 容错性更好，允许消息沿着不同路径传播，避免单点故障

### ⽆主复制
* 放弃主库的概念，并允许任何副本直接接受来⾃客户端的写⼊。
* Amazon Dynamo, Cassandra, Riak, Voldmort

### 当节点故障时写⼊数据库

#### 读修复和反熵
* Read repair
  * 当客户端并⾏读取多个节点时，它可以检测到任何陈旧的响应。例如，在图5-10中，⽤户2345获得了来⾃Replica 3的版本6值和来⾃副本1和2的版本7值。客户端发现副本3具有陈旧值，并将新值写回复制
品。这种⽅法适⽤于频繁阅读的值。
* Anti-entropy process
  * ⼀些数据存储具有后台进程，该进程不断查找副本之间的数据差异，并将任何缺少的数据从⼀个
副本复制到另⼀个副本。与基于领导者的复制中的复制⽇志不同，此反熵过程不会以任何特定的顺序复
制写⼊，并且在复制数据之前可能会有显着的延迟。

#### 读写的法定⼈数

## 第六章 分区
* 分区的主要目的是为了可扩展性。不同的分区可以放在不共享集群的不同节点上，对于在单个分区上运行的查询，每个节点可以独立执行对自己的查询，因此可以通过添加更多节点来扩大查询吞吐量。

### 分区与复制
* 分区通常与复制结合使用，使得每个分区的副本存储在多个节点上，这意味着即使每条记录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。
* 一个节点可能存储多个分区。如果使用主从复制模型，每个分区领导者被分配给一个节点，追随者(从)被分配给其他节点。每个节点可能是某些分区的领导者，同时是其他分区的追随者。

### 键值数据的分区
* 分区的目标是将数据和查询负载均匀分布在各个节点上，如果每个节点公平分享数据和负载，那么理论上10个节点应该能够处理10倍的数据量和10倍的单个节点的读写吞吐量。
* 如果分区时不均匀的，一些分区比其他分区有更多的数据或查询，那么就有偏斜(skew)。在极端情况下，所有负载可能压在一个分区上，这个分区被称为热点(hot spot)。

#### 根据键的范围分区
* 为每个分区指定一块连续的键范围

#### 根据键的散列分区
* 由于偏斜和热点的风险，许多分布式存储使用散列函数来确定给定键的分区。
* 一致性哈希(consistent hashing)
  * 能均匀分配负载，它是由随机选择的分区边界(partition boundaries)来避免中央控制或分布式一致性的需要。

#### 负载倾斜与消除热点
* 大多数数据库系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜。

### 分片与次级索引
#### 按文档的二级索引
#### 根据关键词的二级索引

### 分区再平衡
* 随着时间推移，数据库会发生变化
  * 查询吞吐量增加，需要添加更多CPU
  * 数据量增加，需要更多RAM和Disk
  * 机器出现故障，需要新机器代替它
* 将负载从集群的一个节点向另一个节点移动的过程称为再平衡。
  * 再平衡之后，负载应该再集群中的节点之间公平地共享
  * 再平衡发生时，数据库应该正常接收读取和写入
  * 节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘I/O负载

#### 平衡策略
##### 固定数量的分区
* 创建比节点更多的分区，并为每个节点分配多个分区，这样新节点可以从当前每个节点窃取一些分区，直到分区再次公平分配。

##### 动态分区
* 对于是由键范围分区的数据库，具有固定边界的固定数量的分区将非常不便，如果出现边界错误，则可能会导致一个分区中的所有数据或者其他分区的所有数据为空。
* 按键的范围进行分区的数据库会动态创建分区。当分区增长到超过配置的大小时，会被分成两个分区，每个分区约占一半的数据。对应地，数据删除后如果分区缩小到阈值以下，则可以将其与相邻分区合并。与B树Node Merge发生的过程类似。

##### 按节点⽐例分区
* 每个节点具有固定数量的分区。在这种情况下，每个分区的⼤⼩与数据集⼤⼩成⽐例地增⻓，⽽节点数量保持不变，但是当增加节点数时，分区将再次变⼩。由于较⼤的数据量通常需要较⼤数量的节点进⾏存
储，因此这种⽅法也使每个分区的⼤⼩较为稳定。

### 请求路由
* 服务发现
  * 允许客户端连接任何节点，通过轮询的负载均衡。
  * 由一个路由层来负责。
  * 要求客户端知道分区的节点的分配。

#### 执⾏并⾏查询

## 第七章 事务
* 数据系统出错原因
  * 数据库软件、硬件随时可能故障
  * 应用程序随时会崩溃
  * 网络中断
  * 多个客户端同时操作数据库，覆盖彼此的更改
  * 客户端可能读取到无意义的数据，因为数据只更新了一部分
  * 客户之间的竞争条件可能导致令人惊讶的错误
### 事务的棘⼿概念
#### ACID的含义
##### 原⼦性（Atomicity）
* 当客户想进⾏多次写⼊，但在⼀些写操作处理完之后出现故障的情况。例如进程
崩溃，⽹络连接中断，磁盘变满或者某种完整性约束被违反。如果这些写操作被分组到⼀个原⼦事务
中，并且该事务由于错误⽽不能完成（提交），则该事务将被中⽌，并且数据库必须丢弃或撤消该事务
中迄今为⽌所做的任何写⼊。
* 能够在错误时中⽌事务，丢弃该事务进⾏的所有写⼊变更的能⼒。

##### ⼀致性（Consistency）
* 对数据的⼀组特定陈述必须始终成⽴。即不变量（invariants）
* ⼀致性的这种概念取决于应⽤程序对不变量的观念，应⽤程序负责正确定义它的事务，并保持⼀
致性。这并不是数据库可以保证的事情：如果你写⼊违反不变量的脏数据，数据库也⽆法阻⽌你。

##### 隔离性（Isolation）
* 同时执⾏的事务是相互隔离的：它们不能相互冒犯。传统的数据库教科书
将隔离性形式化为可序列化（Serializability），这意味着每个事务可以假装它是唯⼀在整个数据库上
运⾏的事务。数据库确保当事务已经提交时，结果与它们按顺序运⾏（⼀个接⼀个）是⼀样的，尽管实
际上它们可能是并发运⾏的。

##### 持久性（Durability）
* 持久性是⼀个承诺，即⼀旦事务成功完成，即使发⽣硬件故障或数据库崩溃，写⼊的任何数据也不会丢失。

#### 单对象和多对象操作
##### 单对象写⼊
* 当单个对象发⽣改变时，原⼦性和隔离也是适⽤的。
* ⼀些数据库也提供更复杂的原⼦操作，例如⾃增操作，这样就不再需要像 图7-1 那样的读取-修改-写⼊
序列了。同样流⾏的是 ⽐较和设置（CAS, compare-and-set） 操作，当值没有并发被其他⼈修改过
时，才允许执⾏写操作。
##### 多对象事务的需求
* 许多分布式数据存储已经放弃了多对象事务，因为多对象事务很难跨分区实现，⽽且在需要⾼可⽤性或
⾼性能的情况下，它们可能会碍事。但说到底，在分布式数据库中实现事务，并没有什么根本性的障
碍。
* 是否有可能只⽤键值数据模型和单对象操作来实现任何应⽤程序
  * 在关系数据模型中，⼀个表中的⾏通常具有对另⼀个表中的⾏的外键引⽤。 （类似的是，在⼀个
图数据模型中，⼀个顶点有着到其他顶点的边）。多对象事务使你确信这些引⽤始终有效：当插⼊
⼏个相互引⽤的记录时，外键必须是正确的，最新的，不然数据就没有意义。
  * 在⽂档数据模型中，需要⼀起更新的字段通常在同⼀个⽂档中，这被视为单个对象——更新单个⽂
档时不需要多对象事务。但是，缺乏连接功能的⽂档数据库会⿎励⾮规范化（参阅“关系型数据库
与⽂档数据库在今⽇的对⽐”）。当需要更新⾮规范化的信息时，如 图7-2 所示，需要⼀次更新多
个⽂档。事务在这种情况下⾮常有⽤，可以防⽌⾮规范化的数据不同步。
  * 在具有⼆级索引的数据库中（除了纯粹的键值存储以外⼏乎都有），每次更改值时都需要更新索
引。从事务⻆度来看，这些索引是不同的数据库对象：例如，如果没有事务隔离性，记录可能出现
在⼀个索引中，但没有出现在另⼀个索引中，因为第⼆个索引的更新还没有发⽣。

##### 处理错误和中⽌
* 事务的⼀个关键特性是，如果发⽣错误，它可以中⽌并安全地重试。 ACID数据库基于这样的哲学：如
果数据库有违反其原⼦性，隔离性或持久性的危险，则宁愿完全放弃事务，⽽不是留下半成品。
* 尽管重试⼀个中⽌的事务是⼀个简单⽽有效的错误处理机制，但它并不完美。
  * 如果事务实际上成功了，但是在服务器试图向客户端确认提交成功时⽹络发⽣故障（所以客户端认
为提交失败了），那么重试事务会导致事务被执⾏两次——除⾮你有⼀个额外的应⽤级除重机制。
  * 如果错误是由于负载过⼤造成的，则重试事务将使问题变得更糟，⽽不是更好。为了避免这种正反
馈循环，可以限制重试次数，使⽤指数退避算法，并单独处理与过载相关的错误（如果允许）。
  * 仅在临时性错误（例如，由于死锁，异常情况，临时性⽹络中断和故障切换）后才值得重试。在发
⽣永久性错误（例如，违反约束）之后重试是毫⽆意义的。
  * 如果事务在数据库之外也有副作⽤，即使事务被中⽌，也可能发⽣这些副作⽤。例如，如果你正在
发送电⼦邮件，那你肯定不希望每次重试事务时都重新发送电⼦邮件。如果你想确保⼏个不同的系
统⼀起提交或放弃，⼆阶段提交（2PC, two-phase commit） 可以提供帮助（“原⼦提交和两阶
段提交（2PC）”中将讨论这个问题）。
  * 如果客户端进程在重试中失效，任何试图写⼊数据库的数据都将丢失。

### 弱隔离级别
* 数据库⼀直试图通过提供事务隔离（transaction isolation） 来隐藏应⽤程序开发者
的并发问题。从理论上讲，隔离可以通过假装没有并发发⽣，让你的⽣活更加轻松：可序列化
（serializable） 的隔离等级意味着数据库保证事务的效果与连续运⾏（即⼀次⼀个，没有任何并发）
是⼀样的。
#### 读已提交（Read Committed）
* 从数据库读时，只能看到已提交的数据（没有脏读（dirty reads））
* 写⼊数据库时，只会覆盖已经写⼊的数据（没有脏写（dirty writes））
  
##### 实现读已提交
* 数据库通过使⽤⾏锁（row-level lock） 来防⽌脏写：当事务想要修改特定对象（⾏
或⽂档）时，它必须⾸先获得该对象的锁。然后必须持有该锁直到事务被提交或中⽌。⼀次只有⼀个事
务可持有任何给定对象的锁；如果另⼀个事务要写⼊同⼀个对象，则必须等到第⼀个事务提交或中⽌
后，才能获取该锁并继续。这种锁定是读已提交模式（或更强的隔离级别）的数据库⾃动完成的。
* ⼤多数数据库对于写⼊的每个对象，数据库都会记住旧
的已提交值，和由当前持有写⼊锁的事务设置的新值。 当事务正在进⾏时，任何其他读取对象的事务都
会拿到旧值。 只有当新值提交后，事务才会切换到读取新值。

#### 快照隔离和可重复读
* 为不可重复读(nonrepeatable read)/读取偏差(read skew)

##### 备份
* 进⾏备份需要复制整个数据库，对⼤型数据库⽽⾔可能需要花费数⼩时才能完成。备份进程运⾏时，数
据库仍然会接受写⼊操作。因此备份可能会包含⼀些旧的部分和⼀些新的部分。如果从这样的备份中恢
复，那么不⼀致（如消失的钱）就会变成永久的。

##### 分析查询和完整性检查
* 有时，您可能需要运⾏⼀个查询，扫描⼤部分的数据库。这样的查询在分析中很常⻅（，也可能是定期完整性检查（即监视数据损坏）的⼀部分。如果这些查询在不同时间点观察数据库的不同部分，则可能会返回毫⽆意义的结果。
* 快照隔离（snapshot isolation）是这个问题最常⻅的解决⽅案。想法是，每个事务都从数据库
的⼀致快照（consistent snapshot） 中读取——也就是说，事务可以看到事务开始时在数据库中提交
的所有数据。即使这些数据随后被另⼀个事务更改，每个事务也只能看到该特定时间点的旧数据。
快照隔离对⻓时间运⾏的只读查询（如备份和分析）⾮常有⽤。如果查询的数据在查询执⾏的同时发⽣
变化，则很难理解查询的含义。当⼀个事务可以看到数据库在某个特定时间点冻结时的⼀致快照，理解
起来就很容易了。

#### 实现快照隔离
* 与读取提交的隔离类似，快照隔离的实现通常使⽤写锁来防⽌脏写（请参阅“读已提交”），这意味着进
⾏写⼊的事务会阻⽌另⼀个事务修改同⼀个对象。但是读取不需要任何锁定。从性能的⻆度来看，快照
隔离的⼀个关键原则是：读不阻塞写，写不阻塞读。这允许数据库在处理⼀致性快照上的⻓时间查询
时，可以正常地同时处理写⼊操作。且两者间没有任何锁定争⽤。
* 为了实现快照隔离，数据库使⽤了我们看到的⽤于防⽌图7-4中的脏读的机制的⼀般化。数据库必须可
能保留⼀个对象的⼏个不同的提交版本，因为各种正在进⾏的事务可能需要看到数据库在不同的时间点
的状态。因为它并排维护着多个版本的对象，所以这种技术被称为多版本并发控制（MVCC, multi-version concurrentcy control）。
* 如果⼀个数据库只需要提供读已提交的隔离级别，⽽不提供快照隔离，那么保留⼀个对象的两个版本就
⾜够了：提交的版本和被覆盖但尚未提交的版本。⽀持快照隔离的存储引擎通常也使⽤MVCC来实现读
已提交隔离级别。⼀种典型的⽅法是读已提交为每个查询使⽤单独的快照，⽽快照隔离对整个事务使⽤
相同的快照。

#### 观察⼀致性快照的可见性规则


### 可序列化
* 读已提交和快照隔离级别会阻⽌某些竞争条件，但不会阻⽌另⼀些。我们遇到了⼀些特别棘⼿的例⼦，写⼊偏差和幻读。
  * 隔离级别难以理解，并且在不同的数据库中的实现不一致。
  * 光检查应⽤代码很难判断在特定的隔离级别运⾏是否安全。 特别是在⼤型应⽤程序中，您可能并
不知道并发发⽣的所有事情。
  * 没有检测竞争条件的好⼯具。原则上来说，静态分析可能会有帮助，但研究中的技术还没
法实际应⽤。并发问题的测试是很难的，因为它们通常是⾮确定性的 —— 只有在倒霉的时机下才
会出现问题。
* 可序列化（Serializability）隔离通常被认为是最强的隔离级别。它保证即使事务可以并⾏执⾏，最终
的结果也是⼀样的，就好像它们没有任何并发性，连续挨个执⾏⼀样。因此数据库保证，如果事务在单
独运⾏时正常运⾏，则它们在并发运⾏时继续保持正确 —— 换句话说，数据库可以防⽌所有可能的竞
争条件。
  * 字⾯意义上地串⾏顺序执⾏事务。
  * 两相锁定（2PL, two-phase locking），⼏⼗年来唯⼀可⾏的选择。
  * 乐观并发控制技术，例如可序列化的快照隔离（serializable snapshot isolation）。

#### 真的串⾏执⾏
* 只使用单线程，在单个线程上按顺序⼀次只执⾏⼀个事务。这样做就完
全绕开了检测/防⽌事务间冲突的问题，由此产⽣的隔离，正是可序列化的定义。
  * RAM⾜够便宜了，许多场景现在都可以将完整的活跃数据集保存在内存中。（参阅“在内存中存储
⼀切”）。当事务需要访问的所有数据都在内存中时，事务处理的执⾏速度要⽐等待数据从磁盘加
载时快得多。
  * 数据库设计⼈员意识到OLTP事务通常很短，⽽且只进⾏少量的读写操作（参阅“事务处理或分
析？”）。相⽐之下，⻓时间运⾏的分析查询通常是只读的，因此它们可以在串⾏执⾏循环之外的
⼀致快照（使⽤快照隔离）上运⾏。
* 串⾏执⾏事务的⽅法在VoltDB/H-Store，Redis和Datomic中实现【46,47,48】。设计⽤于单线程执⾏
的系统有时可以⽐⽀持并发的系统更好，因为它可以避免锁的协调开销。但是其吞吐量仅限于单个CPU
核的吞吐量。为了充分利⽤单⼀线程，需要与传统形式不同的结构的事务。

#### 在存储过程中封装事务
* 如果整个过程是⼀个事务，那么它就可以被原⼦化地执⾏。
* 如果数据库事务需要等待来⾃⽤户的输⼊，则数据库
需要⽀持潜在的⼤量并发事务，其中⼤部分是空闲的。⼤多数数据库不能⾼效完成这项⼯作，因此⼏乎
所有的OLTP应⽤程序都避免在事务中等待交互式的⽤户输⼊，以此来保持事务的简短。在Web上，这
意味着事务在同⼀个HTTP请求中被提交——⼀个事务不会跨越多个请求。⼀个新的HTTP请求开始⼀个
新的事务。
* 即使⼈类已经找到了关键路径，事务仍然以交互式的客户端/服务器⻛格执⾏，⼀次⼀个语句。应⽤程序
进⾏查询，读取结果，可能根据第⼀个查询的结果进⾏另⼀个查询，依此类推。查询和结果在应⽤程序
代码（在⼀台机器上运⾏）和数据库服务器（在另⼀台机器上）之间来回发送。
* 在这种交互式的事务⽅式中，应⽤程序和数据库之间的⽹络通信耗费了⼤量的时间。如果不允许在数据
库中进⾏并发处理，且⼀次只处理⼀个事务，则吞吐量将会⾮常糟糕，因为数据库⼤部分的时间都花费
在等待应⽤程序发出当前事务的下⼀个查询。在这种数据库中，为了获得合理的性能，需要同时处理多
个事务。
* 出于这个原因，具有单线程串⾏事务处理的系统不允许交互式的多语句事务。取⽽代之，应⽤程序必须
提前将整个事务代码作为存储过程提交给数据库。这些⽅法之间的差异如图7-9 所示。如果事务所需的
所有数据都在内存中，则存储过程可以⾮常快地执⾏，⽽不⽤等待任何⽹络或磁盘I/O。

#### 存储过程的优点和缺点
* 每个数据库⼚商都有⾃⼰的存储过程语⾔（Oracle有PL/SQL，SQL Server有T-SQL，PostgreSQL
有PL/pgSQL等）。这些语⾔并没有跟上通⽤编程语⾔的发展，所以从今天的⻆度来看，它们看起
来相当丑陋和陈旧，⽽且缺乏⼤多数编程语⾔中能找到的库的⽣态系统。
* 与应⽤服务器相，⽐在数据库中运⾏的管理困难，调试困难，版本控制和部署起来也更为尴尬，更
难测试，更难和⽤于监控的指标收集系统相集成。
* 数据库通常⽐应⽤服务器对性能敏感的多，因为单个数据库实例通常由许多应⽤服务器共享。数据
库中⼀个写得不好的存储过程（例如，占⽤⼤量内存或CPU时间）会⽐在应⽤服务器中相同的代码
造成更多的麻烦。
* 存储过程与内存存储，使得在单个线程上执⾏所有事务变得可⾏。由于不需要等待I/O，且避免了并发
控制机制的开销，它们可以在单个线程上实现相当好的吞吐量。

#### 分区
* 顺序执⾏所有事务使并发控制简单多了，但数据库的事务吞吐量被限制为单机单核的速度。只读事务可
以使⽤快照隔离在其它地⽅执⾏，但对于写⼊吞吐量较⾼的应⽤，单线程事务处理器可能成为⼀个严重
的瓶颈。
* 为了扩展到多个CPU核⼼和多个节点，可以对数据进⾏分区（参⻅第6章），在VoltDB中⽀持这样做。
如果你可以找到⼀种对数据集进⾏分区的⽅法，以便每个事务只需要在单个分区中读写数据，那么每个
分区就可以拥有⾃⼰独⽴运⾏的事务处理线程。在这种情况下可以为每个分区指派⼀个独⽴的CPU核，
事务吞吐量就可以与CPU核数保持线性扩展。
* 但是，对于需要访问多个分区的任何事务，数据库必须在触及的所有分区之间协调事务。存储过程需要
跨越所有分区锁定执⾏，以确保整个系统的可串⾏性。
* 由于跨分区事务具有额外的协调开销，所以它们⽐单分区事务慢得多。 VoltDB报告的吞吐量⼤约是每
秒1000个跨分区写⼊，⽐单分区吞吐量低⼏个数量级，并且不能通过增加更多的机器来增加。
* 事务是否可以是划分⾄单个分区很⼤程度上取决于应⽤数据的结构。简单的键值数据通常可以⾮常容易
地进⾏分区，但是具有多个⼆级索引的数据可能需要⼤量的跨分区协调。

#### 串⾏执⾏⼩结
* 在特定约束条件下，真的串⾏执⾏事务，已经成为⼀种实现可序列化隔离等级的可⾏办法。
  * 每个事务都必须⼩⽽快，只要有⼀个缓慢的事务，就会拖慢所有事务处理。
  * 仅限于活跃数据集可以放⼊内存的情况。很少访问的数据可能会被移动到磁盘，但如果需要在单线
程执⾏的事务中访问，系统就会变得⾮常慢。
  * 写⼊吞吐量必须低到能在单个CPU核上处理，如若不然，事务需要能划分⾄单个分区，且不需要跨
分区协调。
  * 跨分区事务是可能的，但是它们的使⽤程度有很⼤的限制。

### 两阶段锁定（2PL，two-phase locking）
* 只要没有写⼊，就允许多个事务同时读取同⼀个对象。但对象只要有写⼊（修改或删除），就需要独占访问（exclusive access） 权限。
  * 如果事务A读取了⼀个对象，并且事务B想要写⼊该对象，那么B必须等到A提交或中⽌才能继续。
（这确保B不能在A底下意外地改变对象。）
  * 如果事务A写⼊了⼀个对象，并且事务B想要读取该对象，则B必须等到A提交或中⽌才能继续。
* 在2PL中，写⼊不仅会阻塞其他写⼊，也会阻塞读，反之亦然。快照隔离使得读不阻塞写，写也不阻塞
读（参阅“实现快照隔离”），这是2PL和快照隔离之间的关键区别。另⼀⽅⾯，因为2PL提供了可序列化
的性质，它可以防⽌早先讨论的所有竞争条件，包括丢失更新和写⼊偏差。
#### 实现两阶段锁
* 读与写的阻塞是通过为数据库中每个对象添加锁来实现的。锁可以处于共享模式（shared mode）或
独占模式（exclusive mode）。锁使⽤如下
  * 若事务要读取对象，则须先以共享模式获取锁。允许多个事务同时持有共享锁。但如果另⼀个事务
已经在对象上持有排它锁，则这些事务必须等待。
  * 若事务要写⼊⼀个对象，它必须⾸先以独占模式获取该锁。没有其他事务可以同时持有锁（⽆论是
共享模式还是独占模式），所以如果对象上存在任何锁，该事务必须等待。
  * 如果事务先读取再写⼊对象，则它可能会将其共享锁升级为独占锁。升级锁的⼯作与直接获得排他
锁相同。
  * 事务获得锁之后，必须继续持有锁直到事务结束（提交或中⽌）。这就是“两阶段”这个名字的来
源：第⼀阶段（当事务正在执⾏时）获取锁，第⼆阶段（在事务结束时）释放所有的锁。
* 数据库会⾃动检测事务之间的死锁，并中⽌其中⼀个，以便另⼀个继续执⾏。被中
⽌的事务需要由应⽤程序重试。
#### 两阶段锁定的性能
* 由于获取和释放所有这些锁的开销，但更重要的是由于并发性的降低。按照设计，如果两个
并发事务试图做任何可能导致竞争条件的事情，那么必须等待另⼀个完成。
* 传统的关系数据库不限制事务的持续时间，因为它们是为等待⼈类输⼊的交互式应⽤⽽设计的。因此，
当⼀个事务需要等待另⼀个事务时，等待的时⻓并没有限制。即使你保证所有的事务都很短，如果有多
个事务想要访问同⼀个对象，那么可能会形成⼀个队列，所以事务可能需要等待⼏个其他事务才能完
成。
* 因此，运⾏2PL的数据库可能具有相当不稳定的延迟，如果在⼯作负载中存在争⽤，那么可能⾼百分位
点处的响应会⾮常的慢。可能只需要⼀个缓慢的事务，或者⼀个访问⼤量数据并获
取许多锁的事务，就能把系统的其他部分拖慢，甚⾄迫使系统停机。当需要稳健的操作时，这种不稳定
性是有问题的。
* 基于锁实现的读已提交隔离级别可能发⽣死锁，但在基于2PL实现的可序列化隔离级别中，它们会出现
的频繁的多（取决于事务的访问模式）。这可能是⼀个额外的性能问题：当事务由于死锁⽽被中⽌并被
重试时，它需要从头重做它的⼯作。如果死锁很频繁，这可能意味着巨⼤的浪费。
#### 谓词锁
* 幻读（phantoms）的问题。即⼀个事务改变另⼀个事务的搜索查询的结果。具有可序列化隔离级别的
数据库必须防⽌幻读。
* 在会议室预订的例⼦中，这意味着如果⼀个事务在某个时间窗⼝内搜索了⼀个房间的现有预订（⻅例7-
2），则另⼀个事务不能同时插⼊或更新同⼀时间窗⼝与同⼀房间的另⼀个预订 （可以同时插⼊其他房
间的预订，或在不影响另⼀个预定的条件下预定同⼀房间的其他时间段）。
* 如何实现这⼀点？从概念上讲，我们需要⼀个谓词锁（predicate lock）【3】。它类似于前⾯描述的
共享/排它锁，但不属于特定的对象（例如，表中的⼀⾏），它属于所有符合某些搜索条件的对象，如
  * SELECT * FROM bookings
 WHERE room_id = 123 AND
 end_time > '2018-01-01 12:00' AND
 start_time < '2018-01-01 13:00';
* 如果事务A想要读取匹配某些条件的对象，就像在这个 SELECT 查询中那样，它必须获取查询条件
上的共享谓词锁（shared-mode predicate lock）。如果另⼀个事务B持有任何满⾜这⼀查询条
件对象的排它锁，那么A必须等到B释放它的锁之后才允许进⾏查询。
* 如果事务A想要插⼊，更新或删除任何对象，则必须⾸先检查旧值或新值是否与任何现有的谓词锁
匹配。如果事务B持有匹配的谓词锁，那么A必须等到B已经提交或中⽌后才能继续。
#### 索引范围锁
* 不幸的是谓词锁性能不佳：如果活跃事务持有很多锁，检查匹配的锁会⾮常耗时。因此，⼤多数使⽤
2PL的数据库实际上实现了索引范围锁（也称为间隙锁（next-key locking）），这是⼀个简化的近似
版谓词锁。
* 通过使谓词匹配到⼀个更⼤的集合来简化谓词锁是安全的。例如，如果你有在中午和下午1点之间预订
123号房间的谓词锁，则锁定123号房间的所有时间段，或者锁定12:00~13:00时间段的所有房间（不只
是123号房间）是⼀个安全的近似，因为任何满⾜原始谓词的写⼊也⼀定会满⾜这种更松散的近似。
* 通过使谓词匹配到⼀个更⼤的集合来简化谓词锁是安全的。例如，如果你有在中午和下午1点之间预订
123号房间的谓词锁，则锁定123号房间的所有时间段，或者锁定12:00~13:00时间段的所有房间（不只
是123号房间）是⼀个安全的近似，因为任何满⾜原始谓词的写⼊也⼀定会满⾜这种更松散的近似。
* 在房间预订数据库中，您可能会在 room_id 列上有⼀个索引，并且/或者在 start_time 和 end_time
上有索引。
  * 假设您的索引位于 room_id 上，并且数据库使⽤此索引查找123号房间的现有预订。现在数据库可
以简单地将共享锁附加到这个索引项上，指示事务已搜索123号房间⽤于预订。
  * 或者，如果数据库使⽤基于时间的索引来查找现有预订，那么它可以将共享锁附加到该索引中的⼀
系列值，指示事务已经将12:00~13:00时间段标记为⽤于预定。
* ⽆论哪种⽅式，搜索条件的近似值都附加到其中⼀个索引上。现在，如果另⼀个事务想要插⼊，更新或
删除同⼀个房间和/或重叠时间段的预订，则它将不得不更新索引的相同部分。在这样做的过程中，它会
遇到共享锁，它将被迫等到锁被释放。
* 这种⽅法能够有效防⽌幻读和写⼊偏差。索引范围锁并不像谓词锁那样精确（它们可能会锁定更⼤范围
的对象，⽽不是维持可串⾏化所必需的范围），但是由于它们的开销较低，所以是⼀个很好的折衷。
* 如果没有可以挂载间隙锁的索引，数据库可以退化到使⽤整个表上的共享锁。这对性能不利，因为它会
阻⽌所有其他事务写⼊表格，但这是⼀个安全的回退位置。

### 序列化快照隔离（SSI, serializable snapshot isolation）
* 它提供了完整的可序列化隔离级别，但与快照隔离相⽐只有只有很⼩的性能损失。
#### 悲观与乐观的并发控制
* 两阶段锁是⼀种所谓的悲观并发控制机制（pessimistic） ：它是基于这样的原则：如果有事情可能出
错（如另⼀个事务所持有的锁所表示的），最好等到情况安全后再做任何事情。这就像互斥，⽤于保护
多线程编程中的数据结构。
* 相⽐之下，序列化快照隔离是⼀种乐观（optimistic） 的并发控制技术。在这种情况下，乐观意味着，
如果存在潜在的危险也不阻⽌事务，⽽是继续执⾏事务，希望⼀切都会好起来。当⼀个事务想要提交
时，数据库检查是否有什么不好的事情发⽣（即隔离是否被违反）；如果是的话，事务将被中⽌，并且
必须重试。只有可序列化的事务才被允许提交。

#### 基于过时前提的决策
* 事务从数据库读取⼀些数据，检查查询的结果，并根据它看到的结果决定采取⼀些操作（写⼊数据库）。但是，
在快照隔离的情况下，原始查询的结果在事务提交时可能不再是最新的，因为数据可能在同⼀时间被修
改。
* 事务基于⼀个前提（premise） 采取⾏动（事务开始时候的事实，例如：“⽬前有两名医⽣
正在值班”）。之后当事务要提交时，原始数据可能已经改变——前提可能不再成⽴。
* 当应⽤程序进⾏查询时（例如，“当前有多少医⽣正在值班？”），数据库不知道应⽤逻辑如何使⽤该查
询结果。在这种情况下为了安全，数据库需要假设任何对该结果集的变更都可能会使该事务中的写⼊变
得⽆效。 换⽽⾔之，事务中的查询与写⼊可能存在因果依赖。为了提供可序列化的隔离级别，如果事务
在过时的前提下执⾏操作，数据库必须能检测到这种情况，并中⽌事务。
* 数据库如何知道查询结果是否可能已经改变？有两种情况需要考虑
  * 检测对旧MVCC对象版本的读取（读之前存在未提交的写⼊）
  * 检测影响先前读取的写⼊（读之后发⽣写⼊）
* 快照隔离通常是通过多版本并发控制（MVCC；⻅图7-10）来实现的。当⼀个事务从MVCC
数据库中的⼀致快照读时，它将忽略取快照时尚未提交的任何其他事务所做的写⼊。在图7-10中，事务
43 认为Alice的 on_call = true ，因为事务42（修改Alice的待命状态）未被提交。然⽽，在事务43
想要提交时，事务42 已经提交。这意味着在读⼀致性快照时被忽略的写⼊已经⽣效，事务43 的前提不
再为真。
* 为了防⽌这种异常，数据库需要跟踪⼀个事务由于MVCC可⻅性规则⽽忽略另⼀个事务的写⼊。当事务
想要提交时，数据库检查是否有任何被忽略的写⼊现在已经被提交。如果是这样，事务必须中⽌。
* 为什么要等到提交？当检测到陈旧的读取时，为什么不⽴即中⽌事务43 ？因为如果事务43 是只读事
务，则不需要中⽌，因为没有写⼊偏差的⻛险。当事务43 进⾏读取时，数据库还不知道事务是否要稍
后执⾏写操作。此外，事务42 可能在事务43 被提交的时候中⽌或者可能仍然未被提交，因此读取可能
终究不是陈旧的。通过避免不必要的中⽌，SSI 保留快照隔离对从⼀致快照中⻓时间运⾏的读取的⽀
持。
#### 检测影响之前读取的写⼊
* 在两阶段锁定的上下⽂中，我们讨论了索引范围锁（请参阅“索引范围锁”），它允许数据库锁定与某个
搜索查询匹配的所有⾏的访问权，例如 WHERE shift_id = 1234 。可以在这⾥使⽤类似的技术，除了
SSI锁不会阻塞其他事务。

#### 可序列化的快照隔离的性能
* 与两阶段锁定相⽐，可序列化快照隔离的最⼤优点是⼀个事务不需要阻塞等待另⼀个事务所持有的锁。
就像在快照隔离下⼀样，写不会阻塞读，反之亦然。这种设计原则使得查询延迟更可预测，变量更少。
特别是，只读查询可以运⾏在⼀致的快照上，⽽不需要任何锁定，这对于读取繁重的⼯作负载⾮常有吸
引⼒。
* 与串⾏执⾏相⽐，可序列化快照隔离并不局限于单个CPU核的吞吐量：FoundationDB将检测到的序列
化冲突分布在多台机器上，允许扩展到很⾼的吞吐量。即使数据可能跨多台机器进⾏分区，事务也可以
在保证可序列化隔离等级的同时读写多个分区中的数据。
* 中⽌率显着影响SSI的整体表现。例如，⻓时间读取和写⼊数据的事务很可能会发⽣冲突并中⽌，因此
SSI要求同时读写的事务尽量短（只读⻓事务可能没问题）。对于慢事务，SSI可能⽐两阶段锁定或串⾏
执⾏更不敏感。

#### 检测旧MVCC读取
## 第八章 分布式系统的麻烦

### 故障与部分失效
* 单个计算机上的软件没有根本性的不可靠原因：当硬件正常⼯作时，相同的操作总是产⽣相同的结果
（这是确定性的）。如果存在硬件问题（例如，内存损坏或连接器松动），其后果通常是整个系统故障
（例如，内核恐慌，“蓝屏死机”，启动失败）。装有良好软件的个⼈计算机通常要么功能完好，要么完
全失效，⽽不是介于两者之间。
* 当你编写运⾏在多台计算机上的软件时，情况有本质上的区别。在分布式系统中，我们不再处于理想化
的系统模型中，我们别⽆选择，只能⾯对现实世界的混乱现实。⽽在现实世界中，各种各样的事情都可
能会出现问题。
* 在分布式系统中，尽管系统的其他部分⼯作正常，但系统的某些部分可能会以某种不可预知的⽅式被破
坏。这被称为部分失效（partial failure）。难点在于部分失效是不确定性的
（nonderterministic）：如果你试图做任何涉及多个节点和⽹络的事情，它有时可能会⼯作，有时会
出现不可预知的失败。正如我们将要看到的，你甚⾄不知道是否成功了，因为消息通过⽹络传播的时间
也是不确定的！

### 云计算与超级计算机
* ⾼性能计算（HPC）领域。具有数千个CPU的超级计算机通常⽤于计算密集型科学计
算任务，如天⽓预报或分⼦动⼒学（模拟原⼦和分⼦的运动）。
* 云计算（cloud computing），云计算并不是⼀个良好定义的概念【6】，但通常与
多租户数据中⼼，连接IP⽹络的商品计算机（通常是以太⽹），弹性/按需资源分配以及计量计费
等相关联。
* 传统企业数据中⼼位于这两个极端之间。
* 不同的哲学会导致不同的故障处理⽅式。在超级计算机中，作业通常会不时地会将计算的状态存盘到持
久存储中。如果⼀个节点出现故障，通常的解决⽅案是简单地停⽌整个集群的⼯作负载。故障节点修复
后，计算从上⼀个检查点重新开始。因此，超级计算机更像是⼀个单节点计算机⽽不是分布式
系统：通过让部分失败升级为完全失败来处理部分失败——如果系统的任何部分发⽣故障，只是让所有
的东⻄都崩溃（就像单台机器上的内核恐慌⼀样）
* 许多与互联⽹有关的应⽤程序都是在线（online）的，因为它们需要能够随时以低延迟服务⽤
户。使服务不可⽤（例如，停⽌群集以进⾏修复）是不可接受的。相⽐之下，像天⽓模拟这样的离
线（批处理）⼯作可以停⽌并重新启动，影响相当⼩。
* 超级计算机通常由专⽤硬件构建⽽成，每个节点相当可靠，节点通过共享内存和远程直接内存访问
（RDMA）进⾏通信。另⼀⽅⾯，云服务中的节点是由商品机器构建⽽成的，由于规模经济，可以
以较低的成本提供相同的性能，⽽且具有较⾼的故障率。
* ⼤型数据中⼼⽹络通常基于IP和以太⽹，以闭合拓扑排列，以提供更⾼的⼆等分带宽。超级
计算机通常使⽤专⻔的⽹络拓扑结构，例如多维⽹格和环⾯，这为具有已知通信模式的
HPC⼯作负载提供了更好的性能。(系统越⼤，其组件之⼀就越有可能发⽣变化。随着时间的推移，破碎的东⻄得到修复，新的东⻄被破坏，但是在⼀个有成千上万个节点的系统中，有理由认为总是有⼀些东⻄被破坏。当错误处理策略由简单的放弃组成时，⼀个⼤的系统最终会花费⼤量时间从错误中恢复，⽽不是做有
⽤的⼯作)。
* 如果系统可以容忍发⽣故障的节点，并继续保持整体⼯作状态，那么这对于操作和维护⾮常有⽤：
例如，可以执⾏滚动升级（参阅第4章），⼀次重新启动⼀个节点，⽽服务继续服务⽤户不中断。
在云环境中，如果⼀台虚拟机运⾏不佳，可以杀死它并请求⼀台新的虚拟机（希望新的虚拟机速度
更快）
* 在地理位置分散的部署中（保持数据在地理位置上接近⽤户以减少访问延迟），通信很可能通过互
联⽹进⾏，与本地⽹络相⽐，通信速度缓慢且不可靠。超级计算机通常假设它们的所有节点都靠近
在⼀起。
* 迟早会有⼀部分系统出现故障，软件必须以某种⽅式处理。故障处理
必须是软件设计的⼀部分，并且作为软件的运维，您需要知道在发⽣故障的情况下，软件可能会表现出
怎样的⾏为。
### 不可靠的⽹络
* ⽆共享并不是构建系统的唯⼀⽅式，但它已经成为构建互联⽹服务的主要⽅式，其原因如下：相对便
宜，因为它不需要特殊的硬件，可以利⽤商品化的云计算服务，通过跨多个地理分布的数据中⼼进⾏冗
余可以实现⾼可靠性。
* 互联⽹和数据中⼼（通常是以太⽹）中的⼤多数内部⽹络都是异步分组⽹络（asynchronous packet
networks）。在这种⽹络中，⼀个节点可以向另⼀个节点发送⼀个消息（⼀个数据包），但是⽹络不
能保证它什么时候到达，或者是否到达。如果您发送请求并期待响应，则很多事情可能会出错。
  * 请求可能已经丢失（可能有⼈拔掉了⽹线）
  * 请求可能正在排队，稍后将交付（也许⽹络或收件⼈超载）
  * 远程节点可能已经失效（可能是崩溃或关机）
  * 远程节点可能暂时停⽌了响应（可能会遇到⻓时间的垃圾回收暂停；参阅“暂停进程”），但稍后会
再次响应。
  * 远程节点可能已经处理了请求，但是⽹络上的响应已经丢失（可能是⽹络交换机配置错误）
  * 远程节点可能已经处理了请求，但是响应已经被延迟，并且稍后将被传递（可能是⽹络或者你⾃⼰
的机器过载）
* 发送者甚⾄不能分辨数据包是否被发送：唯⼀的选择是让接收者发送响应消息，这可能会丢失或延迟。
这些问题在异步⽹络中难以区分：您所拥有的唯⼀信息是，您尚未收到响应。如果您向另⼀个节点发送
请求并且没有收到响应，则⽆法说明原因。
* 处理这个问题的通常⽅法是超时（Timeout）：在⼀段时间之后放弃等待，并且认为响应不会到达。但
是，当发⽣超时时，你仍然不知道远程节点是否收到了请求（如果请求仍然在某个地⽅排队，那么即使
发件⼈已经放弃了该请求，仍然可能会将其发送给收件⼈）。

### 检测故障
* 负载平衡器需要停⽌向已死亡的节点转发请求（即从移出轮询列表（out of rotation））
* 在单主复制功能的分布式数据库中，如果主库失效，则需要将从库之⼀升级为新主库（参
阅“ch5.md#处理节点宕机”）
* ⽹络的不确定性使得很难判断⼀个节点是否⼯作。在某些特定的情况下，您可能会收到⼀些
反馈信息，明确告诉您某些事情没有成功
* 如果你可以到达运⾏节点的机器，但没有进程正在侦听⽬标端⼝（例如，因为进程崩溃），操作系
统将通过发送FIN或RST来关闭并重⽤TCP连接。但是，如果节点在处理请求时发⽣崩溃，则⽆法知
道远程节点实际处理了多少数据
* 如果节点进程崩溃（或被管理员杀死），但节点的操作系统仍在运⾏，则脚本可以通知其他节点有
关该崩溃的信息，以便另⼀个节点可以快速接管，⽽⽆需等待超时到期。
* 如果您有权访问数据中⼼⽹络交换机的管理界⾯，则可以查询它们以检测硬件级别的链路故障（例
如，远程机器是否关闭电源）。如果您通过互联⽹连接，或者如果您处于共享数据中⼼⽽⽆法访问
交换机，或者由于⽹络问题⽽⽆法访问管理界⾯，则排除此选项。
* 如果路由器确认您尝试连接的IP地址不可⽤，则可能会使⽤ICMP⽬标不可达数据包回复您。但
是，路由器不具备神奇的故障检测能⼒——它受到与⽹络其他参与者相同的限制。

### 超时与⽆穷的延迟
* ⻓时间的超时意味着⻓时间等待，直到⼀个节点被宣告死亡（在这段时间内，⽤户可能不得不等待，或
者看到错误信息）。短暂的超时可以更快地检测到故障，但是实际上它只是经历了暂时的减速（例如，
由于节点或⽹络上的负载峰值）⽽导致错误地宣布节点失效的⻛险更⾼。
* 当⼀个节点被宣告死亡时，它的职责需要转移到其他节点，这会给其他节点和⽹络带来额外的负担。如
果系统已经处于⾼负荷状态，则过早宣告节点死亡会使问题更严重。尤其是可能发⽣，节点实际上并没
有死亡，⽽是由于过载导致响应缓慢；将其负载转移到其他节点可能会导致级联失效（cascading
failure）（在极端情况下，所有节点都宣告对⽅死亡，并且所有节点都停⽌⼯作）。
* 不幸的是，我们所使⽤的⼤多数系统都没有这些保证：异步⽹络具有⽆限的延迟（即尽可能快地传送数
据包，但数据包到达可能需要的时间没有上限），并且⼤多数服务器实现并不能保证它们可以在⼀定的
最⼤时间内处理请求（请参阅“响应时间保证”）。对于故障检测，系统⼤部分时间快速运⾏是不够的：
如果你的超时时间很短，往返时间只需要⼀个瞬时尖峰就可以使系统失衡。
#### ⽹络拥塞和排队
* 如果多个不同的节点同时尝试将数据包发送到同⼀⽬的地，则⽹络交换机必须将它们排队并将它们
逐个送⼊⽬标⽹络链路（如图8-2所示）。在繁忙的⽹络链路上，数据包可能需要等待⼀段时间才
能获得⼀个插槽（这称为⽹络连接）。如果传⼊的数据太多，交换机队列填满，数据包将被丢弃，
因此需要重新发送数据包 - 即使⽹络运⾏良好。
* 当数据包到达⽬标机器时，如果所有CPU内核当前都处于繁忙状态，则来⾃⽹络的传⼊请求将被操
作系统排队，直到应⽤程序准备好处理它为⽌。根据机器上的负载，这可能需要⼀段任意的时间。
* 在虚拟化环境中，正在运⾏的操作系统经常暂停⼏⼗毫秒，⽽另⼀个虚拟机使⽤CPU内核。在这段
时间内，虚拟机不能从⽹络中消耗任何数据，所以传⼊的数据被虚拟机监视器排队（缓
冲），进⼀步增加了⽹络延迟的可变性。
* TCP执⾏流量控制（flow control）（也称为拥塞避免（congestion avoidance）或背压
（backpressure）），其中节点限制⾃⼰的发送速率以避免⽹络链路或接收节点过载。这
意味着在数据甚⾄进⼊⽹络之前，在发送者处需要进⾏额外的排队。
### 同步⽹络 vs 异步⽹络
* 如果我们可以依靠⽹络来传递⼀些最⼤延迟固定的数据包，⽽不是丢弃数据包，那么分布式系统就会简
单得多。
* 这种⽹络是同步的：即使数据经过多个路由器，也不会受到排队的影响，因为呼叫的16位空间已经在⽹
络的下⼀跳中保留了下来。⽽且由于没有排队，⽹络的最⼤端到端延迟是固定的。我们称之为有限延迟
（bounded delay）

### 不可靠的时钟
* 在分布式系统中，时间是⼀件棘⼿的事情，因为通信不是即时的：消息通过⽹络从⼀台机器传送到另⼀
台机器需要时间。收到消息的时间总是晚于发送的时间，但是由于⽹络中的可变延迟，我们不知道多少
时间。这个事实有时很难确定在涉及多台机器时发⽣事情的顺序。
* ⽽且，⽹络上的每台机器都有⾃⼰的时钟，这是⼀个实际的硬件设备：通常是⽯英晶体振荡器。这些设
备不是完全准确的，所以每台机器都有⾃⼰的时间概念，可能⽐其他机器稍快或更慢。可以在⼀定程度
上同步时钟：最常⽤的机制是⽹络时间协议（NTP），它允许根据⼀组服务器报告的时间来调整计算机
时钟。服务器则从更精确的时间源（如GPS接收机）获取时间。

### 单调钟与时钟
#### 时钟
* 时钟是您直观地了解时钟的依据：它根据某个⽇历（也称为挂钟时间（wall-clock time））返回当前
⽇期和时间。例如，Linux 5 上的 clock_gettime(CLOCK_REALTIME) 和Java中的
System.currentTimeMillis() 返回⾃epoch（1970年1⽉1⽇ 午夜 UTC，格⾥⾼利历）以来的秒数
（或毫秒），根据公历⽇历，不包括闰秒。有些系统使⽤其他⽇期作为参考点。
* 时钟通常与NTP同步，这意味着来⾃⼀台机器的时间戳（理想情况下）意味着与另⼀台机器上的时间戳
相同。但是如下节所述，时钟也具有各种各样的奇特之处。特别是，如果本地时钟在NTP服务器之前太
远，则它可能会被强制重置，看上去好像跳回了先前的时间点。这些跳跃以及他们经常忽略闰秒的事
实，使时钟不能⽤于测量经过时间
#### 单调钟
* 单调钟适⽤于测量持续时间（时间间隔），例如超时或服务的响应时间：Linux上的
clock_gettime(CLOCK_MONOTONIC) ，和Java中的 System.nanoTime() 都是单调时钟。这个名字来
源于他们保证总是前进的事实（⽽时钟可以及时跳回）

### 时钟同步与准确性

#### 依赖同步时钟
* 时钟的问题在于，虽然它们看起来简单易⽤，但却具有令⼈惊讶的缺陷：⼀天可能不会有精确的86,400
秒，时钟可能会前后跳跃，⽽⼀个节点上的时间可能与另⼀个节点上的时间完全不同。
* 有序事件的时间戳

#### 时钟读数存在置信区间

### 暂停进程
#### 响应时间保证

## 第九章 一致性与共识
* 分布式系统最重要的抽象之⼀就是共识（consensus）：就是让所有的节点对某件事达成⼀致。正如我们
在本章中将会看到的那样，尽管存在⽹络故障和流程故障，可靠地达成共识是⼀个令⼈惊讶的棘⼿问
题
### ⼀致性保证
* 最终⼀致性/收敛：如果你停⽌向数据库写⼊数据并等待⼀段不确定
的时间，那么最终所有的读取请求都会返回相同的值。换句话说，不⼀致性是暂时的，最终会⾃
⾏解决（假设⽹络中的任何故障最终都会被修复）。
* 然⽽，这是⼀个⾮常弱的保证 —— 它并没有说什么什么时候副本会收敛。在收敛之前，读操作可能会
返回任何东⻄或什么都没。例如，如果你写⼊了⼀个值，然后⽴即再次读取，这并不能保证你
能看到刚跟写⼊的值，因为读请求可能会被路由到另外的副本上。

### 线性⼀致性(linearizability)
* 在最终⼀致的数据库，如果你在同⼀时刻问两个不同副本相同的问题，可能会得到两个不同的答案。这
很让⼈困惑。如果数据库可以提供只有⼀个副本的假象（即，只有⼀个数据副本），那么事情就简单太
多了。那么每个客户端都会有相同的数据视图，且不必担⼼复制滞后了。
* 在⼀个线性⼀致的系统中，只要⼀个客户端成功完成写操作，所有客户端从数据库中读取数据必须能够
看到刚刚写⼊的值。维护数据的单个副本的错觉是指，系统能保障读到的值是最近的，最新的，⽽不是
来⾃陈旧的缓存或副本。换句话说，线性⼀致性是⼀个新鲜度保证（recency guarantee）。
* 线性⼀致性的要求是，操作标记的连线总是按时间（从左到右）向前移动，⽽不是向后移动。这个要求
确保了我们之前讨论的新鲜性保证：⼀旦新的值被写⼊或读取，所有后续的读都会看到写⼊的值，直到
它被再次覆盖。

### 依赖线性⼀致性
* 锁定和领导选举
* 约束和唯⼀性保证
* 跨信道的时序依赖

### 实现线性⼀致的系统
* 单主复制（可能线性⼀致）
* 共识算法（线性⼀致）
* 多主复制（⾮线性⼀致）
* ⽆主复制（也许不是线性⼀致的）

### 线性⼀致性的代价
#### CAP定理
* 如果应⽤需要线性⼀致性，且某些副本因为⽹络问题与其他副本断开连接，那么这些副本掉线时不
能处理请求。请求必须等到⽹络问题解决，或直接返回错误。（⽆论哪种⽅式，服务都不可⽤
（unavailable））。
* 如果应⽤不需要线性⼀致性，那么某个副本即使与其他副本断开连接，也可独⽴处理请求（例如
多主复制）。在这种情况下，应⽤可以在⽹络问题前保持可⽤，但其⾏为不是线性⼀致的。
* 因此不需要线性⼀致性的应⽤对⽹络问题有更强的容错能⼒。这种⻅解通常被称为CAP定理。

#### 线性⼀致性和⽹络延迟
* 虽然线性⼀致是⼀个很有⽤的保证，但实际上，线性⼀致的系统惊⼈的少。例如，现代多核CPU上的内
存甚⾄都不是线性⼀致的：如果⼀个CPU核上运⾏的线程写⼊某个内存地址，⽽另⼀个CPU核上
运⾏的线程不久之后读取相同的地址，并没有保证⼀定能⼀定读到第⼀个线程写⼊的值（除⾮使⽤了内
存屏障（memory barrier）或围栏（fence））。
* 这种⾏为的原因是每个CPU核都有⾃⼰的内存缓存和存储缓冲区。默认情况下，内存访问⾸先⾛缓存，
任何变更会异步写⼊主存。因为缓存访问⽐主存要快得多【45】，所以这个特性对于现代CPU的良好性
能表现⾄关重要。但是现在就有⼏个数据副本（⼀个在主存中，也许还有⼏个在不同缓存中的其他副
本），⽽且这些副本是异步更新的，所以就失去了线性⼀致性。
* 为什么要做这个权衡？对多核内存⼀致性模型⽽⾔，CAP定理是没有意义的：在同⼀台计算机中，我们
通常假定通 信都是可靠的。并且我们并不指望⼀个CPU核能在脱离计算机其他部分的条件下继续正常⼯
作。牺牲线性⼀致性的原因是性能（performance），⽽不是容错。
* 许多分布式数据库也是如此：它们是为了提⾼性能⽽选择了牺牲线性⼀致性，⽽不是为了容错。
线性⼀致的速度很慢——这始终是事实，⽽不仅仅是⽹络故障期间。
* 如果你
想要线性⼀致性，读写请求的响应时间⾄少与⽹络延迟的不确定性成正⽐。在像⼤多数计算机⽹络⼀样
具有⾼度可变延迟的⽹络中（参⻅“超时与⽆穷的延迟”），线性读写的响应时间不可避免地会很⾼。更
快地线性⼀致算法不存在，但更弱的⼀致性模型可以快得多，所以对延迟敏感的系统⽽⾔，这类权衡⾮
常重要。

### 顺序保证
### 顺序与因果
* 顺序反复出现有⼏个原因，其中⼀个原因是，它有助于保持因果关系（causality）。

#### 因果顺序不是全序的
* 全序（total order）允许任意两个元素进⾏⽐较，所以如果有两个元素，你总是可以说出哪个更⼤，
哪个更⼩。例如，⾃然数集是全序的：给定两个⾃然数，⽐如说5和13，那么你可以告诉我，13⼤于
5。
* 然⽽数学集合并不完全是全序的： {a, b} ⽐ {b, c} 更⼤吗？好吧，你没法真正⽐较它们，因为⼆者
都不是对⽅的⼦集。我们说它们是⽆法⽐较（incomparable）的，因此数学集合是偏序（partially
order）的：在某些情况下，可以说⼀个集合⼤于另⼀个（如果⼀个集合包含另⼀个集合的所有元
素），但在其他情况下它们是⽆法⽐较的。
* 线性⼀致性
  * 在线性⼀致的系统中，操作是全序的：如果系统表现的就好像只有⼀个数据副本，并且所有操作都是原
  ⼦性的，这意味着对任何两个操作，我们总是能判定哪个操作先发⽣。这个全序图9-4中以时间线表
  示。
* 因果性
  * 如果两个操作都没有在彼此之前发⽣，那么这两个操作是并发的（参阅“此前发⽣”的关系和
并发）。换句话说，如果两个事件是因果相关的（⼀个发⽣在另⼀个事件之前），则它们之间是有序
的，但如果它们是并发的，则它们之间的顺序是⽆法⽐较的。这意味着因果关系定义了⼀个偏序，⽽不
是⼀个全序：⼀些操作相互之间是有顺序的，但有些则是⽆法⽐较的。
* 因此，根据这个定义，在线性⼀致的数据存储中是不存在并发操作的：必须有且仅有⼀条时间线，所有
的操作都在这条时间线上，构成⼀个全序关系。可能有⼏个请求在等待处理，但是数据存储确保了每个
请求都是在唯⼀时间线上的某个时间点⾃动处理的，不存在任何并发。
并发意味着时间线会分岔然后合并 —— 在这种情况下，不同分⽀上的操作是⽆法⽐较的（即并发操
作）。在第五章中我们看到了这种现象：例如，图5-14 并不是⼀条直线的全序关系，⽽是⼀堆不同的操
作并发进⾏。图中的箭头指明了因果依赖 —— 操作的偏序。
如果你熟悉像Git这样的分布式版本控制系统，那么其版本历史与因果关系图极其相似。通常，⼀个提交
（Commit）发⽣在另⼀个提交之后，在⼀条直线上。但是有时你会遇到分⽀（当多个⼈同时在⼀个项
⽬上⼯作时），合并（Merge）会在这些并发创建的提交相融合时创建。

#### 线性⼀致性强于因果⼀致性

#### 捕获因果关系

### 两阶段提交
* 两阶段提交（two-phase commit）是⼀种⽤于实现跨多个节点的原⼦事务提交的算法，即确保所有节
点提交或所有节点中⽌。
* 2PC使⽤⼀个通常不会出现在单节点事务中的新组件：协调者（coordinator）（也称为事务管理器
（transaction manager））。协调者通常在请求事务的相同应⽤进程中以库的形式实现（例如，嵌
⼊在Java EE容器中），但也可以是单ᇿ的进程或服务。这种协调者的例⼦包括Narayana，JOTM，BTM
或MSDTC。
* 正常情况下，2PC事务以应⽤在多个数据库节点上读写数据开始。我们称这些数据库节点为参与者
（participants）。当应⽤准备提交时，协调者开始阶段 1 ：它发送⼀个准备（prepare）请求到每个
节点，询问它们是否能够提交。然后协调者会跟踪参与者的响应。
  * 如果所有参与者都回答“是”，表示它们已经准备好提交，那么协调者在阶段 2 发出提交，（commit）请求，然后提交真正发⽣。
  * 如果任意⼀个参与者回复了“否”，则协调者在阶段2 中向所有节点发送中⽌（abort）请求。

* ⼯作原理
  * 当应⽤想要启动⼀个分布式事务时，它向协调者请求⼀个事务ID。此事务ID是全局唯⼀的。
  * 应⽤在每个参与者上启动单节点事务，并在单节点事务上捎带上这个全局事务ID。所有的读写都是
在这些单节点事务中各⾃完成的。如果在这个阶段出现任何问题（例如，节点崩溃或请求超时），
则协调者或任何参与者都可以中⽌。
  * 当应⽤准备提交时，协调者向所有参与者发送⼀个准备请求，并打上全局事务ID的标记。如果任意
⼀个请求失败或超时，则协调者向所有参与者发送针对该事务ID的中⽌请求。
  * 参与者收到准备请求时，需要确保在任意情况下都的确可以提交事务。这包括将所有事务数据写⼊
磁盘（出现故障，电源故障，或硬盘空间不⾜都不能是稍后拒绝提交的理由）以及检查是否存在任
何冲突或违反约束。通过向协调者回答“是”，节点承诺，只要请求，这个事务⼀定可以不出差错地
提交。换句话说，参与者放弃了中⽌事务的权利，但没有实际提交。
  * 当协调者收到所有准备请求的答复时，会就提交或中⽌事务作出明确的决定（只有在所有参与者投
赞成票的情况下才会提交）。协调者必须把这个决定写到磁盘上的事务⽇志中，如果它随后就崩
溃，恢复后也能知道⾃⼰所做的决定。这被称为提交点（commit point）。
  * ⼀旦协调者的决定落盘，提交或放弃请求会发送给所有参与者。如果这个请求失败或超时，协调者
必须永远保持重试，直到成功为⽌。没有回头路：如果已经做出决定，不管需要多少次重试它都必
须被执⾏。如果参与者在此期间崩溃，事务将在其恢复后提交——由于参与者投了赞成，因此恢复
后它不能拒绝提交。


### 三阶段提交
* 两阶段提交被称为阻塞（blocking）原⼦提交协议，因为存在2PC可能卡住并等待协调者恢复的情况。
理论上，可以使⼀个原⼦提交协议变为⾮阻塞（nonblocking）的，以便在节点失败时不会卡住。但是
让这个协议能在实践中⼯作并没有那么简单。
* 作为2PC的替代⽅案，已经提出了⼀种称为三阶段提交（3PC）的算法【13,80】。然⽽，3PC假定⽹络
延迟有界，节点响应时间有限；在⼤多数具有⽆限⽹络延迟和进程暂停的实际系统中（⻅第8章），它
并不能保证原⼦性。
* 通常，⾮阻塞原⼦提交需要⼀个完美的故障检测器（perfect failure detector）【67,71】—— 即⼀
个可靠的机制来判断⼀个节点是否已经崩溃。在具有⽆限延迟的⽹络中，超时并不是⼀种可靠的故障检
测机制，因为即使没有节点崩溃，请求也可能由于⽹络问题⽽超时。出于这个原因，2PC仍然被使⽤，
尽管⼤家都清楚可能存在协调者故障的问题。

### 实践中的分布式事务
#### 数据库内部的分布式事务
* ⼀些分布式数据库（即在其标准配置中使⽤复制和分区的数据库）⽀持数据库节点之间的内部事务。例
如，VoltDB和MySQL Cluster的NDB存储引擎就有这样的内部事务⽀持。在这种情况下，所有参与事务
的节点都运⾏相同的数据库软件。
#### 异构分布式事务

* 在异构（heterogeneous）事务中，参与者是两种或以上不同技术：例如来⾃不同供应商的两个数据
库，甚⾄是⾮数据库系统（如消息代理）。跨系统的分布式事务必须确保原⼦提交，尽管系统可能完全
不同。
* 数据库内部事务不必与任何其他系统兼容，因此它们可以使⽤任何协议，并能针对特定技术进⾏特定的
优化。因此数据库内部的分布式事务通常⼯作地很好。另⼀⽅⾯，跨异构技术的事务则更有挑战性。
#### 恰好⼀次的消息处理

# 第三部分 衍生数据
* 从其他数据集衍生出一些数据集的系统。衍生数据经常出现在异构系统中，当没有单个数据库可以把所有事情都做的很好时，应用需要集成几种不同的数据库、缓存、索引等。
## 记录和衍⽣数据系统
### 记录系统（System of record）
* 记录系统，也被称为真相源（source of truth），持有数据的权威版本。当新的数据进⼊时（例如，
⽤户输⼊）⾸先会记录在这⾥。每个事实正正好好表示⼀次（表示通常是标准化的
（normalized））。如果其他系统和记录系统之间存在任何差异，那么记录系统中的值是正确的（根
据定义）。
### 衍生数据系统（Derived data systems）
* 衍⽣系统中的数据，通常是另⼀个系统中的现有数据以某种⽅式进⾏转换或处理的结果。如果丢失衍⽣
数据，可以从原始来源重新创建。典型的例⼦是缓存（cache）：如果数据在缓存中，就可以由缓存提
供服务；如果缓存不包含所需数据，则降级由底层数据库提供。⾮规范化的值，索引和物化视图亦属此
类。在推荐系统中，预测汇总数据通常衍⽣⾃⽤户⽇志。
* 从技术上讲，衍⽣数据是冗余的（redundant），因为它重复了已有的信息。但是衍⽣数据对于获得良
好的只读查询性能通常是⾄关重要的。它通常是⾮规范化的。可以从单个源头衍⽣出多个不同的数据
集，使你能从不同的“视⻆”洞察数据。

## 第十章 批处理
* 服务（在线系统）
* 批处理系统（离线系统）
* 流处理系统（准实时系统）

### MapReduce和分布式⽂件系统
* 运⾏MapReduce作业通常不会修改输⼊，除了⽣成输出外没有任何副作⽤。
输出⽂件以连续的⽅式⼀次性写⼊（⼀旦写⼊⽂件，不会修改任何现有的⽂件部分）

### MapReduce作业执⾏
#### Mapper
* Mapper会在每条输⼊记录上调⽤⼀次，其⼯作是从输⼊记录中提取键值。对于每个输⼊，它可以⽣成
任意数量的键值对（包括None）。它不会保留从⼀个输⼊记录到下⼀个记录的任何状态，因此每个记录都是独⽴处理的。
#### Reducer
* MapReduce框架拉取由Mapper⽣成的键值对，收集属于同⼀个键的所有值，并使⽤在这组值列表上迭代调⽤Reducer。 Reducer可以产⽣输出记录（例如相同URL的出现次数）。
#### 分布式执⾏MapReduce
* MapReduce可以在多台机器上并⾏执⾏计算，⽽⽆需编写代码来显式处理并⾏问题。Mapper和Reducer⼀次只能处理⼀条记录；它们不需要知道它们的输⼊
  来⾃哪⾥，或者输出去往什么地⽅，所以框架可以处理在机器之间移动数据的复杂性。
* 它们被实现为传统编程语⾔的函数。在Hadoop MapReduce中，Mapper和Reducer都是实现特定接⼝的Java类。在MongoDB和CouchDB中，Mapper和Reducer都是JavaScript函数
#### MapReduce⼯作流
* 单个MapReduce作业可以解决的问题范围很有限。以⽇志分析为例，单个MapReduce作业可以确定每个URL的⻚⾯浏览次数，但⽆法确定最常⻅的URL，因为这需要第⼆轮排序。
* 因此将MapReduce作业链接成为⼯作流（workflow）中是极为常⻅的，例如，⼀个作业的输出成为下⼀个作业的输⼊。 Hadoop Map-Reduce框架对⼯作流没有特殊⽀持，
  所以这个链是通过⽬录名隐式实现的：第⼀个作业必须将其输出配置为HDFS中的指定⽬录，第⼆个作业必须将其输⼊配置为从同⼀个⽬录。从MapReduce框架的⻆度来看，这是是两个独⽴的作业。
* 只有当作业成功完成后，批处理作业的输出才会被视为有效的（MapReduce会丢弃失败作业的部分输出）。因此，⼯作流中的⼀项作业只有在先前的作业 —— 即⽣产其输⼊的作业 —— 成功完成后才能开
始。为了处理这些作业之间的依赖，有很多针对Hadoop的⼯作流调度器被开发出来，包括Oozie，Azkaban，Luigi，Airflow和Pinball

### Reduce端连接与分组
* 在许多数据集中，⼀条记录与另⼀条记录存在关联是很常⻅的：关系模型中的外键，⽂档模型中的⽂档引⽤或图模型中的边。当你需要同时访问这⼀关联的两侧（持有引⽤的记录与被引⽤的记录）时，连接
就是必须的。（包含引⽤的记录和被引⽤的记录），连接就是必需的。正如第2章所讨论的，⾮规范化可以减少对连接的需求，但通常⽆法将其完全移除。
* 当MapReduce作业被赋予⼀组⽂件作为输⼊时，它读取所有这些⽂件的全部内容；数据库会将这种操
作称为全表扫描。如果你只想读取少量的记录，则全表扫描与索引查询相⽐，代价⾮常⾼昂。但是在分
析查询中（参阅“事务处理或分析？”），通常需要计算⼤量记录的聚合。在这种情况下，特别是如果能
在多台机器上并⾏处理时，扫描整个输⼊可能是相当合理的事情。
### 示例：分析⽤户活动事件
* ⼀个批处理作业中连接的典型例⼦。左侧是事件⽇志，描述登录⽤户在⽹站上做的事情
（称为活动事件（activity events）或点击流数据（clickstream data）），右侧是⽤户数据库。 你
可以将此示例看作是星型模式的⼀部分（参阅“星型和雪花型：分析的模式”）：事件⽇志是事实表，⽤
户数据库是其中的⼀个维度。
* 分析任务可能需要将⽤户活动与⽤户简档相关联：例如，如果档案包含⽤户的年龄或出⽣⽇期，系统就
可以确定哪些⻚⾯更受哪些年龄段的⽤户欢迎。然⽽活动事件仅包含⽤户ID，⽽没有包含完整的⽤户档
案信息。在每个活动事件中嵌⼊这些档案信息很可能会⾮常浪费。因此，活动事件需要与⽤户档案数据
库相连接。
* 实现这⼀连接的最简单⽅法是，逐个遍历活动事件，并为每个遇到的⽤户ID查询⽤户数据库（在远程服
务器上）。这是可能的，但是它的性能可能会⾮常差：处理吞吐量将受限于受数据库服务器的往返时
间，本地缓存的有效性很⼤程度上取决于数据的分布，并⾏运⾏⼤量查询可能会轻易压垮数据库
* 为了在批处理过程中实现良好的吞吐量，计算必须（尽可能）限于单台机器上进⾏。为待处理的每条记
录发起随机访问的⽹络请求实在是太慢了。⽽且，查询远程数据库意味着批处理作业变为⾮确定的
（nondeterministic），因为远程数据库中的数据可能会改变。
* 因此，更好的⽅法是获取⽤户数据库的副本（例如，使⽤ETL进程从数据库备份中提取数据，参阅“数据
仓库”），并将它和⽤户⾏为⽇志放⼊同⼀个分布式⽂件系统中。然后你可以将⽤户数据库存储在HDFS
中的⼀组⽂件中，⽽⽤户活动记录存储在另⼀组⽂件中，并能⽤MapReduce将所有相关记录集中到同
⼀个地⽅进⾏⾼效处理。

### 排序合并连接
* Mapper的⽬的是从每个输⼊记录中提取⼀对键值。在图10-2的情况下，这个键就是⽤户
ID：⼀组Mapper会扫过活动事件（提取⽤户ID作为键，活动事件作为值），⽽另⼀组Mapper将会扫过
⽤户数据库（提取⽤户ID作为键，⽤户的出⽣⽇期作为值）
* 当MapReduce框架通过键对Mapper输出进⾏分区，然后对键值对进⾏排序时，效果是具有相同ID的所
有活动事件和⽤户记录在Reducer输⼊中彼此相邻。 Map-Reduce作业甚⾄可以也让这些记录排序，使
Reducer总能先看到来⾃⽤户数据库的记录，紧接着是按时间戳顺序排序的活动事件 —— 这种技术被称
为⼆次排序（secondary sort）
* 然后Reducer可以容易地执⾏实际的连接逻辑：每个⽤户ID都会被调⽤⼀次Reducer函数，且因为⼆次
排序，第⼀个值应该是来⾃⽤户数据库的出⽣⽇期记录。 Reducer将出⽣⽇期存储在局部变量中，然后
使⽤相同的⽤户ID遍历活动事件，输出已观看⽹址和观看者年龄的结果对。随后的Map-Reduce作业可
以计算每个URL的查看者年龄分布，并按年龄段进⾏聚集。
* 由于Reducer⼀次处理⼀个特定⽤户ID的所有记录，因此⼀次只需要将⼀条⽤户记录保存在内存中，⽽
不需要通过⽹络发出任何请求。这个算法被称为排序合并连接（sort-merge join），因为Mapper的输
出是按键排序的，然后Reducer将来⾃连接两侧的有序记录列表合并在⼀起。

### 把相关数据放在⼀起
* 在排序合并连接中，Mapper和排序过程确保了所有对特定⽤户ID执⾏连接操作的必须数据都被放在同
⼀个地⽅：单次调⽤Reducer的地⽅。预先排好了所有需要的数据，Reducer可以是相当简单的单线程
代码，能够以⾼吞吐量和与低内存开销扫过这些记录。
* 这种架构可以看做，Mapper将“消息”发送给Reducer。当⼀个Mapper发出⼀个键值对时，这个键的作
⽤就像值应该传递到的⽬标地址。即使键只是⼀个任意的字符串（不是像IP地址和端⼝号那样的实际的
⽹络地址），它表现的就像⼀个地址：所有具有相同键的键值对将被传递到相同的⽬标（⼀次Reduce
的调⽤）。
* 使⽤MapReduce编程模型，能将计算的物理⽹络通信层⾯（从正确的机器获取数据）从应⽤逻辑中剥
离出来（获取数据后执⾏处理）。这种分离与数据库的典型⽤法形成了鲜明对⽐，从数据库中获取数据
的请求经常出现在应⽤代码内部】。由于MapReduce能够处理所有的⽹络通信，因此它也避免了
应⽤代码去担⼼部分故障，例如另⼀个节点的崩溃：MapReduce在不影响应⽤逻辑的情况下能透明地
重试失败的任务。

### GROUP BY
* 除了连接之外，“把相关数据放在⼀起”的另⼀种常⻅模式是，按某个键对记录分组（如SQL中的GROUP
BY⼦句）。所有带有相同键的记录构成⼀个组，⽽下⼀步往往是在每个组内进⾏某种聚合操作，例如：
  * 统计每个组中记录的数量（例如在统计PV的例⼦中，在SQL中表示为 COUNT(*) 聚合）
  * 对某个特定字段求和（SQL中的 SUM(fieldname) ）
  * 按某种分级函数取出排名前k条记录。
* 使⽤MapReduce实现这种分组操作的最简单⽅法是设置Mapper，以便它们⽣成的键值对使⽤所需的分
组键。然后分区和排序过程将所有具有相同分区键的记录导向同⼀个Reducer。因此在MapReduce之上
实现分组和连接看上去⾮常相似。
* 分组的另⼀个常⻅⽤途是整理特定⽤户会话的所有活动事件，以找出⽤户进⾏的⼀系列操作（称为会话
化（sessionization）【37】）。例如，可以使⽤这种分析来确定显示新版⽹站的⽤户是否⽐那些显示
旧版本（A/B测试）的⽤户更有购买欲，或者计算某个营销活动是否值得。
* 如果你有多个Web服务器处理⽤户请求，则特定⽤户的活动事件很可能分散在各个不同的服务器的⽇志
⽂件中。你可以通过使⽤会话cookie，⽤户ID或类似的标识符作为分组键，以将特定⽤户的所有活动事
件放在⼀起来实现会话化，与此同时，不同⽤户的事件仍然散步在不同的分区中。

### 处理倾斜
* 如果存在与单个键关联的⼤量数据，则“将具有相同键的所有记录放到相同的位置”这种模式就被破坏
了。例如在社交⽹络中，⼤多数⽤户可能会与⼏百⼈有连接，但少数名⼈可能有数百万的追随者。这种
不成⽐例的活动数据库记录被称为关键对象（linchpin object）或热键（hot key）
* 在单个Reducer中收集与某个名流相关的所有活动（例如他们发布内容的回复）可能导致严重的倾斜
（也称为热点（hot spot））—— 也就是说，⼀个Reducer必须⽐其他Reducer处理更多的记录（参
⻅“负载倾斜与消除热点“）。由于MapReduce作业只有在所有Mapper和Reducer都完成时才完成，所
有后续作业必须等待最慢的Reducer才能启动。
* 如果连接的输⼊存在热点键，可以使⽤⼀些算法进⾏补偿。例如，Pig中的倾斜连接（skewed join）⽅
法⾸先运⾏⼀个抽样作业来确定哪些键是热键。连接实际执⾏时，Mapper会将热键的关联记录
随机（相对于传统MapReduce基于键散列的确定性⽅法）发送到⼏个Reducer之⼀。对于另外⼀侧的连
接输⼊，与热键相关的记录需要被复制到所有处理该键的Reducer上。
* 这种技术将处理热键的⼯作分散到多个Reducer上，这样可以使其更好地并⾏化，代价是需要将连接另
⼀侧的输⼊记录复制到多个Reducer上。 Crunch中的分⽚连接（sharded join）⽅法与之类似，但需
要显式指定热键⽽不是使⽤采样作业。这种技术也⾮常类似于我们在“负载倾斜与消除热点”中讨论的技
术，使⽤随机化来缓解分区数据库中的热点。
* Hive的偏斜连接优化采取了另⼀种⽅法。它需要在表格元数据中显式指定热键，并将与这些键相关的记
录单独存放，与其它⽂件分开。当在该表上执⾏连接时，对于热键，它会使⽤Map端连接
* 当按照热键进⾏分组并聚合时，可以将分组分两个阶段进⾏。第⼀个MapReduce阶段将记录发送到随
机Reducer，以便每个Reducer只对热键的⼦集执⾏分组，为每个键输出⼀个更紧凑的中间聚合结果。
然后第⼆个MapReduce作业将所有来⾃第⼀阶段Reducer的中间聚合结果合并为每个键⼀个值。

### Map端连接
* 上⼀节描述的连接算法在Reducer中执⾏实际的连接逻辑，因此被称为Reduce端连接。Mapper扮演着
预处理输⼊数据的⻆⾊：从每个输⼊记录中提取键值，将键值对分配给Reducer分区，并按键排序。
* Reduce端⽅法的优点是不需要对输⼊数据做任何假设：⽆论其属性和结构如何，Mapper都可以对其预
处理以备连接。然⽽不利的⼀⾯是，排序，复制⾄Reducer，以及合并Reducer输⼊，所有这些操作可
能开销巨⼤。当数据通过MapReduce 阶段时，数据可能需要落盘好⼏次，取决于可⽤的内存缓冲区。
* 另⼀⽅⾯，如果你能对输⼊数据作出某些假设，则通过使⽤所谓的Map端连接来加快连接速度是可⾏
的。这种⽅法使⽤了⼀个阉掉Reduce与排序的MapReduce作业，每个Mapper只是简单地从分布式⽂
件系统中读取⼀个输⼊⽂件块，然后将输出⽂件写⼊⽂件系统，仅此⽽已。

#### ⼴播散列连接
* 适⽤于执⾏Map端连接的最简单场景是⼤数据集与⼩数据集连接的情况。要点在于⼩数据集需要⾜够
⼩，以便可以将其全部加载到每个Mapper的内存中。
* 例如，假设在图10-2的情况下，⽤户数据库⼩到⾜以放进内存中。在这种情况下，当Mapper启动时，
它可以⾸先将⽤户数据库从分布式⽂件系统读取到内存中的散列中。完成此操作后，Map程序可以扫描
⽤户活动事件，并简单地在散列表中查找每个事件的⽤户ID。
* 参与连接的较⼤输⼊的每个⽂件块各有⼀个Mapper（在图10-2的例⼦中活动事件是较⼤的输⼊）。每
个Mapper都会将较⼩输⼊整个加载到内存中。
* 这种简单有效的算法被称为⼴播散列连接（broadcast hash join）：⼴播⼀词反映了这样⼀个事实，
每个连接较⼤输⼊端分区的Mapper都会将较⼩输⼊端数据集整个读⼊内存中（所以较⼩输⼊实际上“⼴ 播”到较⼤数据的所有分区上），散列⼀词反映了它使⽤⼀个散列表。 Pig（名为“复制链接
（replicated join）”），Hive（“MapJoin”），Cascading和Crunch⽀持这种连接。它也被诸如
Impala的数据仓库查询引擎使⽤
* 除了将连接较⼩输⼊加载到内存散列表中，另⼀种⽅法是将较⼩输⼊存储在本地磁盘上的只读索引中
【42】。索引中经常使⽤的部分将保留在操作系统的⻚⾯缓存中，因⽽这种⽅法可以提供与内存散列表
⼏乎⼀样快的随机查找性能，但实际上并不需要数据集能放⼊内存中。

#### 分区散列连接
* 如果Map端连接的输⼊以相同的⽅式进⾏分区，则散列连接⽅法可以独⽴应⽤于每个分区。在图10-2的
情况中，你可以根据⽤户ID的最后⼀位⼗进制数字来对活动事件和⽤户数据库进⾏分区（因此连接两侧
各有10个分区）。例如，Mapper3⾸先将所有具有以3结尾的ID的⽤户加载到散列表中，然后扫描ID为 3的每个⽤户的所有活动事件。
* 如果分区正确⽆误，可以确定的是，所有你可能需要连接的记录都落在同⼀个编号的分区中。因此每个
Mapper只需要从输⼊两端各读取⼀个分区就⾜够了。好处是每个Mapper都可以在内存散列表中少放点
数据。
* 这种⽅法只有当连接两端输⼊有相同的分区数，且两侧的记录都是使⽤相同的键与相同的哈希函数做分
区时才适⽤。如果输⼊是由之前执⾏过这种分组的MapReduce作业⽣成的，那么这可能是⼀个合理的
假设。
* 分区散列连接在Hive中称为Map端桶连接（bucketed map joins）

#### Map端合并连接
* 如果输⼊数据集不仅以相同的⽅式进⾏分区，⽽且还基于相同的键进⾏排序，则可适⽤另⼀种Map端联
接的变体。在这种情况下，输⼊是否⼩到能放⼊内存并不重要，因为这时候Mapper同样可以执⾏归并
操作（通常由Reducer执⾏）的归并操作：按键递增的顺序依次读取两个输⼊⽂件，将具有相同键的记
录配对。
* 如果能进⾏Map端合并连接，这通常意味着前⼀个MapReduce作业可能⼀开始就已经把输⼊数据做了
分区并进⾏了排序。原则上这个连接就可以在前⼀个作业的Reduce阶段进⾏。但使⽤独⽴的仅Map作
业有时也是合适的，例如，分好区且排好序的中间数据集可能还会⽤于其他⽬的。
* 当下游作业使⽤MapReduce连接的输出时，选择Map端连接或Reduce端连接会影响输出的结构。
Reduce端连接的输出是按照连接键进⾏分区和排序的，⽽Map端连接的输出则按照与较⼤输⼊相同的
⽅式进⾏分区和排序（因为⽆论是使⽤分区连接还是⼴播连接，连接较⼤输⼊端的每个⽂件块都会启动
⼀个Map任务）
#### MapReduce⼯作流与Map端连接
* 如前所述，Map端连接也对输⼊数据集的⼤⼩，有序性和分区⽅式做出了更多假设。在优化连接策略
时，了解分布式⽂件系统中数据集的物理布局变得⾮常重要：仅仅知道编码格式和数据存储⽬录的名称
是不够的；你还必须知道数据是按哪些键做的分区和排序，以及分区的数量。
* 在Hadoop⽣态系统中，这种关于数据集分区的元数据通常在HCatalog和Hive Metastore中维护。

### 批处理⼯作流的输出
* 我们已经说了很多⽤于实现MapReduce⼯作流的算法，但却忽略了⼀个重要的问题：这些处理完成之
后的最终结果是什么？我们最开始为什么要跑这些作业？
* 在数据库查询的场景中，我们将事务处理（OLTP）与分析两种⽬的区分开来（参阅“事务处理或分
析？”）。我们看到，OLTP查询通常根据键查找少量记录，使⽤索引，并将其呈现给⽤户（⽐如在⽹⻚
上）。另⼀⽅⾯，分析查询通常会扫描⼤量记录，执⾏分组与聚合，输出通常有着报告的形式：显示某
个指标随时间变化的图表，或按照某种排位取前10项，或⼀些数字细化为⼦类。这种报告的消费者通常
是需要做出商业决策的分析师或经理。
* 批处理放哪⾥合适？它不属于事务处理，也不是分析。它和分析⽐较接近，因为批处理通常会扫过输⼊
数据集的绝⼤部分。然⽽MapReduce作业⼯作流与⽤于分析⽬的的SQL查询是不同的（参阅“Hadoop
与分布式数据库的对⽐”）。批处理过程的输出通常不是报表，⽽是⼀些其他类型的结构。

#### 建⽴搜索索引
* Google最初使⽤MapReduce是为其搜索引擎建⽴索引，⽤了由5到10个MapReduce作业组成的⼯作流
实现。虽然Google后来也不仅仅是为这个⽬的⽽使⽤MapReduce，但如果从构建搜索索
引的⻆度来看，更能帮助理解MapReduce。 （直⾄今⽇，Hadoop MapReduce仍然是为Lucene/Solr
构建索引的好⽅法
* 我们在“全⽂搜索和模糊索引”中简要地了解了Lucene这样的全⽂搜索索引是如何⼯作的：它是⼀个⽂件
（关键词字典），你可以在其中⾼效地查找特定关键字，并找到包含该关键字的所有⽂档ID列表（⽂章
列表）。这是⼀种⾮常简化的看法 —— 实际上，搜索索引需要各种额外数据，以便根据相关性对搜索
结果进⾏排名，纠正拼写错误，解析同义词等等 —— 但这个原则是成⽴的。
* 如果需要对⼀组固定⽂档执⾏全⽂搜索，则批处理是⼀种构建索引的⾼效⽅法：Mapper根据需要对⽂
档集合进⾏分区，每个Reducer构建该分区的索引，并将索引⽂件写⼊分布式⽂件系统。构建这样的⽂
档分区索引（参阅“分区和⼆级索引”）并⾏处理效果拔群。
* 由于按关键字查询搜索索引是只读操作，因⽽这些索引⽂件⼀旦创建就是不可变的。
* 如果索引的⽂档集合发⽣更改，⼀种选择是定期重跑整个索引⼯作流，并在完成后⽤新的索引⽂件批量
替换以前的索引⽂件。如果只有少量的⽂档发⽣了变化，这种⽅法的计算成本可能会很⾼。但它的优点
是索引过程很容易理解：⽂档进，索引出。
* 另⼀个选择是，可以增量建⽴索引。如第3章中讨论的，如果要在索引中添加，删除或更新⽂档，
Lucene会写新的段⽂件，并在后台异步合并压缩段⽂件。我们将在第11章中看到更多这种增量处理。

#### 键值存储作为批处理输出
* 搜索索引只是批处理⼯作流可能输出的⼀个例⼦。批处理的另⼀个常⻅⽤途是构建机器学习系统，例如
分类器（⽐如垃圾邮件过滤器，异常检测，图像识别）与推荐系统（例如，你可能认识的⼈，你可能感
兴趣的产品或相关的搜索）。
* 这些批处理作业的输出通常是某种数据库：例如，可以通过给定⽤户ID查询该⽤户推荐好友的数据库，
或者可以通过产品ID查询相关产品的数据库。
* 这些数据库需要被处理⽤户请求的Web应⽤所查询，⽽它们通常是独⽴于Hadoop基础设施的。那么批
处理过程的输出如何回到Web应⽤可以查询的数据库中呢？
  * 正如前⾯在连接的上下⽂中讨论的那样，为每条记录发起⼀个⽹络请求，要⽐批处理任务的正常吞
吐量慢⼏个数量级。即使客户端库⽀持批处理，性能也可能很差。
  * MapReduce作业经常并⾏运⾏许多任务。如果所有Mapper或Reducer都同时写⼊相同的输出数据
库，并以批处理的预期速率⼯作，那么该数据库很可能被轻易压垮，其查询性能可能变差。这可能
会导致系统其他部分的运⾏问题。
  * 通常情况下，MapReduce为作业输出提供了⼀个⼲净利落的“全有或全⽆”保证：如果作业成功，
则结果就是每个任务恰好执⾏⼀次所产⽣的输出，即使某些任务失败且必须⼀路重试。如果整个作
业失败，则不会⽣成输出。然⽽从作业内部写⼊外部系统，会产⽣外部可⻅的副作⽤，这种副作⽤
是不能以这种⽅式被隐藏的。因此，你不得不去操⼼部分完成的作业对其他系统可⻅的结果，并需
要理解Hadoop任务尝试与预测执⾏的复杂性。
* 更好的解决⽅案是在批处理作业内创建⼀个全新的数据库，并将其作为⽂件写⼊分布式⽂件系统中作业
的输出⽬录，就像上节中的搜索索引⼀样。这些数据⽂件⼀旦写⼊就是不可变的，可以批量加载到处理
只读查询的服务器中。不少键值存储都⽀持在MapReduce作业中构建数据库⽂件，包括Voldemort
，Terrapin ，ElephantDB和HBase批量加载。
* 构建这些数据库⽂件是MapReduce的⼀种很好⽤法的使⽤⽅法：使⽤Mapper提取出键并按该键排序，
现在已经是构建索引所必需的⼤量⼯作。由于这些键值存储⼤多都是只读的（⽂件只能由批处理作业⼀
次性写⼊，然后就不可变），所以数据结构⾮常简单。⽐如它们就不需要WAL。
* 将数据加载到Voldemort时，服务器将继续⽤旧数据⽂件服务请求，同时将新数据⽂件从分布式⽂件系
统复制到服务器的本地磁盘。⼀旦复制完成，服务器会⾃动将查询切换到新⽂件。如果在这个过程中出
现任何问题，它可以轻易回滚⾄旧⽂件，因为它们仍然存在⽽且不可变。

#### 批处理输出的哲学
* 程序读取输⼊并写⼊输出。在这⼀过程中，输⼊保持不变，任何先前的输出都被新输出完全替换，且没有其他副作⽤。这
意味着你可以随⼼所欲地重新运⾏⼀个命令，略做改动或进⾏调试，⽽不会搅乱系统的状态。
* MapReduce作业的输出处理遵循同样的原理。通过将输⼊视为不可变且避免副作⽤（如写⼊外部数据
库），批处理作业不仅实现了良好的性能，⽽且更容易维护：
  * 如果在代码中引⼊了⼀个错误，⽽输出错误或损坏了，则可以简单地回滚到代码的先前版本，然后
重新运⾏该作业，输出将重新被纠正。或者，甚⾄更简单，你可以将旧的输出保存在不同的⽬录
中，然后切换回原来的⽬录。具有读写事务的数据库没有这个属性：如果你部署了错误的代码，将
错误的数据写⼊数据库，那么回滚代码将⽆法修复数据库中的数据。 （能够从错误代码中恢复的
概念被称为⼈类容错（human fault tolerance））
  * 由于回滚很容易，⽐起在错误意味着不可挽回的伤害的环境，功能开发进展能快很多。这种最⼩化
不可逆性（minimizing irreversibility）的原则有利于敏捷软件开发。
  * 如果Map或Reduce任务失败，MapReduce框架将⾃动重新调度，并在同样的输⼊上再次运⾏它。
如果失败是由代码中的错误造成的，那么它会不断崩溃，并最终导致作业在⼏次尝试之后失败。但
是如果故障是由于临时问题导致的，那么故障就会被容忍。因为输⼊不可变，这种⾃动重试是安全
的，⽽失败任务的输出会被MapReduce框架丢弃。
  * 同⼀组⽂件可⽤作各种不同作业的输⼊，包括计算指标的监控作业可以评估作业的输出是否具有预
期的性质（例如，将其与前⼀次运⾏的输出进⾏⽐较并测量差异）。
  * 与Unix⼯具类似，MapReduce作业将逻辑与布线（配置输⼊和输出⽬录）分离，这使得关注点分
离，可以重⽤代码：⼀个团队可以实现⼀个专注做好⼀件事的作业；⽽其他团队可以决定何时何地
运⾏这项作业。

### Hadoop与分布式数据库的对⽐
* 正如我们所看到的，Hadoop有点像Unix的分布式版本，其中HDFS是⽂件系统，⽽MapReduce是Unix
进程的怪异实现（总是在Map阶段和Reduce阶段运⾏ sort ⼯具）。我们了解了如何在这些原语的基础
上实现各种连接和分组操作。
* MPP数据库专注于在⼀组机器上并⾏执⾏分析SQL查询，⽽MapReduce和分布式⽂件
系统的组合则更像是⼀个可以运⾏任意程序的通⽤操作系统。

#### 存储多样性
* 数据库要求你根据特定的模型（例如关系或⽂档）来构造数据，⽽分布式⽂件系统中的⽂件只是字节序
列，可以使⽤任何数据模型和编码来编写。它们可能是数据库记录的集合，但同样可以是⽂本，图像，
视频，传感器读数，稀疏矩阵，特征向量，基因组序列或任何其他类型的数据。
* 说⽩了，Hadoop开放了将数据不加区分地转储到HDFS的可能性，允许后续再研究如何进⼀步处理
。相⽐之下，在将数据导⼊数据库专有存储格式之前，MPP数据库通常需要对数据和查询模式进
⾏仔细的前期建模。
* 在纯粹主义者看来，这种仔细的建模和导⼊似乎是可取的，因为这意味着数据库的⽤户有更⾼质量的数
据来处理。然⽽实践经验表明，简单地使数据快速可⽤ —— 即使它很古怪，难以使⽤，使⽤原始格式
—— 也通常要⽐事先决定理想数据模型要更有价值。
* 这个想法与数据仓库类似（参阅“数据仓库”）：将⼤型组织的各个部分的数据集中在⼀起是很有价值
的，因为它可以跨越以前相分离的数据集进⾏连接。 MPP数据库所要求的谨慎模式设计拖慢了集中式数
据收集速度；以原始形式收集数据，稍后再操⼼模式的设计，能使数据收集速度加快（有时被称为“数据
湖（data lake）”或“企业数据中⼼（enterprise data hub）”）。
* 不加区分的数据转储转移了解释数据的负担：数据集的⽣产者不再需要强制将其转化为标准格式，数据
的解释成为消费者的问题（读时模式⽅法【56】；参阅“⽂档模型中的架构灵活性”）。如果⽣产者和消
费者是不同优先级的不同团队，这可能是⼀种优势。甚⾄可能不存在⼀个理想的数据模型，对于不同⽬
的有不同的合适视⻆。以原始形式简单地转储数据，可以允许多种这样的转换。这种⽅法被称为寿司原
则（sushi principle）：“原始数据更好”。
* 因此，Hadoop经常被⽤于实现ETL过程（参阅“数据仓库”）：事务处理系统中的数据以某种原始形式转
储到分布式⽂件系统中，然后编写MapReduce作业来清理数据，将其转换为关系形式，并将其导⼊
MPP数据仓库以进⾏分析。数据建模仍然在进⾏，但它在⼀个单独的步骤中进⾏，与数据收集相解耦。
这种解耦是可⾏的，因为分布式⽂件系统⽀持以任何格式编码的数据。

#### 处理模型多样性
* MPP数据库是单体的，紧密集成的软件，负责磁盘上的存储布局，查询计划，调度和执⾏。由于这些组
件都可以针对数据库的特定需求进⾏调整和优化，因此整个系统可以在其设计针对的查询类型上取得⾮
常好的性能。⽽且，SQL查询语⾔允许以优雅的语法表达查询，⽽⽆需编写代码，使业务分析师⽤来做
商业分析的可视化⼯具（例如Tableau）能够访问。
* 另⼀⽅⾯，并⾮所有类型的处理都可以合理地表达为SQL查询。例如，如果要构建机器学习和推荐系
统，或者使⽤相关性排名模型的全⽂搜索索引，或者执⾏图像分析，则很可能需要更⼀般的数据处理模
型。这些类型的处理通常是特别针对特定应⽤的（例如机器学习的特征⼯程，机器翻译的⾃然语⾔模
型，欺诈预测的⻛险评估函数），因此它们不可避免地需要编写代码，⽽不仅仅是查询。
* MapReduce使⼯程师能够轻松地在⼤型数据集上运⾏⾃⼰的代码。如果你有HDFS和MapReduce，那
么你可以在它之上建⽴⼀个SQL查询执⾏引擎，事实上这正是Hive项⽬所做的【31】。但是，你也可以
编写许多其他形式的批处理，这些批处理不必⾮要⽤SQL查询表示。
* 随后，⼈们发现MapReduce对于某些类型的处理⽽⾔局限性很⼤，表现很差，因此在Hadoop之上其他
各种处理模型也被开发出来（我们将在“MapReduce之后”中看到其中⼀些）。有两种处理模型，SQL和
MapReduce，还不够，需要更多不同的模型！⽽且由于Hadoop平台的开放性，实施⼀整套⽅法是可⾏
的，⽽这在单体MPP数据库的范畴内是不可能的
* ⾄关重要的是，这些不同的处理模型都可以在共享的单个机器集群上运⾏，所有这些机器都可以访问分
布式⽂件系统上的相同⽂件。在Hadoop⽅法中，不需要将数据导⼊到⼏个不同的专⽤系统中进⾏不同
类型的处理：系统⾜够灵活，可以⽀持同⼀个群集内不同的⼯作负载。不需要移动数据，使得从数据中
挖掘价值变得容易得多，也使采⽤新的处理模型容易的多。
* Hadoop⽣态系统包括随机访问的OLTP数据库，如HBase（参阅“SSTables和LSM树”）和MPP⻛格的分
析型数据库，如Impala 【41】。 HBase与Impala都不使⽤MapReduce，但都使⽤HDFS进⾏存储。它
们是迥异的数据访问与处理⽅法，但是它们可以共存，并被集成到同⼀个系统中。

#### 针对频繁故障设计
* 当⽐较MapReduce和MPP数据库时，两种不同的设计思路出现了：处理故障和使⽤内存与磁盘的⽅
式。与在线系统相⽐，批处理对故障不太敏感，因为就算失败也不会⽴即影响到⽤户，⽽且它们总是能
再次运⾏。
* 如果⼀个节点在执⾏查询时崩溃，⼤多数MPP数据库会中⽌整个查询，并让⽤户重新提交查询或⾃动重
新运⾏它【3】。由于查询通常最多运⾏⼏秒钟或⼏分钟，所以这种错误处理的⽅法是可以接受的，因
为重试的代价不是太⼤。 MPP数据库还倾向于在内存中保留尽可能多的数据（例如，使⽤散列连接）以
避免从磁盘读取的开销。
* 另⼀⽅⾯，MapReduce可以容忍单个Map或Reduce任务的失败，⽽不会影响作业的整体，通过以单个
任务的粒度重试⼯作。它也会⾮常急切地将数据写⼊磁盘，⼀⽅⾯是为了容错，另⼀部分是因为假设数
据集太⼤⽽不能适应内存。
* MapReduce⽅式更适⽤于较⼤的作业：要处理如此之多的数据并运⾏很⻓时间的作业，以⾄于在此过
程中很可能⾄少遇到⼀个任务故障。在这种情况下，由于单个任务失败⽽重新运⾏整个作业将是⾮常浪
费的。即使以单个任务的粒度进⾏恢复引⼊了使得⽆故障处理更慢的开销，但如果任务失败率⾜够⾼，
这仍然是⼀种合理的权衡。
* 但是这些假设有多么现实呢？在⼤多数集群中，机器故障确实会发⽣，但是它们不是很频繁 —— 可能
少到绝⼤多数作业都不会经历机器故障。为了容错，真的值得带来这么⼤的额外开销吗？
* 要了解MapReduce节约使⽤内存和在任务的层次进⾏恢复的原因，了解最初设计MapReduce的环境是
很有帮助的。 Google有着混⽤的数据中⼼，在线⽣产服务和离线批处理作业在同样机器上运⾏。每个
任务都有⼀个通过容器强制执⾏的资源配给（CPU核⼼，RAM，磁盘空间等）。每个任务也具有优先
级，如果优先级较⾼的任务需要更多的资源，则可以终⽌（抢占）同⼀台机器上较低优先级的任务以释
放资源。优先级还决定了计算资源的定价：团队必须为他们使⽤的资源付费，⽽优先级更⾼的进程花费
更多。
* 这种架构允许⾮⽣产（低优先级）计算资源被过量使⽤（overcommitted），因为系统知道必要时它
可以回收资源。与分离⽣产和⾮⽣产任务的系统相⽐，过量使⽤资源可以更好地利⽤机器并提⾼效率。
但由于MapReduce作业以低优先级运⾏，它们随时都有被抢占的⻛险，因为优先级较⾼的进程可能需
要其资源。在⾼优先级进程拿⾛所需资源后，批量作业能有效地“捡⾯包屑”，利⽤剩下的任何计算资
源。
* 在⾕歌，运⾏⼀个⼩时的MapReduce任务有⼤约有5％的⻛险被终⽌，为了给更⾼优先级的进程挪地
⽅。这⼀概率⽐硬件问题，机器重启或其他原因的概率⾼了⼀个数量级【59】。按照这种抢占率，如果
⼀个作业有100个任务，每个任务运⾏10分钟，那么⾄少有⼀个任务在完成之前被终⽌的⻛险⼤于
50％。
* 这就是MapReduce被设计为容忍频繁意外任务终⽌的原因：不是因为硬件很不可靠，⽽是因为任意终
⽌进程的⾃由有利于提⾼计算集群中的资源利⽤率。
* 在开源的集群调度器中，抢占的使⽤较少。 YARN的CapacityScheduler⽀持抢占，以平衡不同队列的资
源分配【58】，但在编写本⽂时，YARN，Mesos或Kubernetes不⽀持通⽤优先级抢占【60】。在任务
不经常被终⽌的环境中，MapReduce的这⼀设计决策就没有多少意义了。在下⼀节中，我们将研究⼀
些与MapReduce设计决策相异的替代⽅案。

### MapReduce之后
## 第十一章 流处理
## 第十二章 数据系统的未来